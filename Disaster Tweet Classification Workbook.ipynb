{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------\n",
    "\n",
    "\n",
    "# Leveraging Social Media To Alert Emergency Response Personnel During Disasters\n",
    "\n",
    "### Contents:\n",
    "-------------------------------\n",
    "- [Importing Libraries](#Importing-Libraries)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Pulling Tweets with Twython](#Pulling-Tweets-with-Twython)\n",
    "- [Filtering for Disaster Tweets: Figure Eight Model](#Filtering-for-Disaster-Tweets:-Figure-Eight-Model)\n",
    "- [Filtering for Disaster Tweets: Crisis Words](#Filtering-for-Disaster-Tweets:-Crisis-Words)\n",
    "- [Apply Filters to Generate List of Disaster Tweets](#Apply-Filters-to-Generate-List-of-Disaster-Tweets)\n",
    "- [Identifying Critical Tweets Among Disaster Tweets](#Identifying-Critical-Tweets-Among-Disaster-Tweets)\n",
    "- [Testing 2-Phase Model (1. Disaster Filter, 2. Predict Critical) on New Data](#Testing-2-Phase-Model-{1.-Disaster-Filter,-2.-Predict-Critical}-on-New-Data)\n",
    "- [Create Combined Landfall Dataset with Class Labeling](#Create-Combined-Landfall-Dataset-with-Class-Labeling)\n",
    "- [Geomapping Landfall Data in Tableau](#Geomapping-Landfall-Data-in-Tableau)\n",
    "- [Live Streaming Disaster Tweets via Twython](#Live-Streaming-Disaster-Tweets-via-Twython)\n",
    "- [Conclusions:](#Conclusions)\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a collaborative, client-oriented project that I worked on with [Javier Martinez](https://www.linkedin.com/in/javier-martinez-abrego/) and [Alexander Nguyen](https://www.linkedin.com/in/alexander-james-nguyen/), at the request of an organization involved in contracting work for FEMA (Federal Emergency Management Agency).  This work was not compensated, and is not proprietary. \n",
    "\n",
    "Our goal in this project was to make initial steps toward designing and implementing a web-tool or an app for tracking developments during a disastrous event, in close to real time.  While traditional methods for alerting on such events rely on official information derived from official sources (e.g. USGS), we were tasked here with attempting to utilize social media activity to identify these events and alert when an event first occurs.  The question we look at primarily here is, given a sea of text content from social media platforms, how do you identify what is relevant information for emergency response personnel?  And what sort of implementation would be valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twython import Twython\n",
    "from tqdm import *\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial plan was to try and real-time map incoming tweets using Twitter, Facebook, Instagram, Snapchat, etc.  We quickly found that the APIs for these social media platforms have become much more restrictive than they used to be.  The limits on how many tweets we could get at one time were such that it wasn't feasible to build out a dataset that would support training a model.  We also could not get geolocation.  We needed a different approach.\n",
    "\n",
    "We found a useful dataset on the website CrisisLex, which collects datasets specific to NLP applications in disaster scenarios. This dataset contained tweet IDs for all geotagged tweets (6 million +) from affected areas of the Eastern Seaboard during the 11 day period surrounding Hurricane Sandy's landfall (10/22/2012-11/2/2012).  The tweets include all content, not just disaster-related tweets.  \n",
    "\n",
    "You can use specific tweet IDs to pull dictionaries corresponding to the specific tweets.  Having specific tweet IDs also allows you to collect up to 900 tweets every fifteen minutes, which is much larger than the normal limit.  The fact that geotags with precise coordinates of sending location and timestamps to the second are included in the data is also very relevant, as it allows live-mapping, and accurately simulates the kind of information we would expect FEMA to have access to in this type of real-world application scenario.\n",
    "\n",
    "Hurricanes, of all disasters, are probably the best-equipped to represent a generalizable lexicon, as hurricanes often involve a combination of flooding, fires, building damage/collapse, sufficient wind to down trees, explosions, injuries, deaths, trapped/stranded individuals, etc.  Obviously, the training data could be expanded to include a variety of disaster types in the future.\n",
    "\n",
    "Further Information on the CrisisLex Sandy Tweet ID Dataset:\n",
    "\n",
    "- [CrisisLex: SandyHurricaneGeoT1 Geo-Located tweets from the 2012 Sandy Hurricane](https://crisislex.org/data-collections.html#SandyHurricaneGeoT1)\n",
    "- Contents: tweet ids for 6,556,328 tweets, representing all tweets from October 22nd, 2012 ‚Äîthe day Sandy formed‚Äî until November 2nd, 2012 ‚Äî the day that it dissipated.\n",
    "- Sampling method: tweets were geotagged and located in Washington DC or one of 13 US states affected by Sandy: Connecticut, Delaware, Massachusetts, Maryland, New Jersey, New York, North Carolina, Ohio, Pennsylvania, Rhode Island, South Carolina, Virginia,West Virginia. This filter was based on a set of bounding boxes that covered the desired area, which also covered small parts of adjacent states.\n",
    "- Labels: no labels. The corpus contains tweets both relevant and irrelevant to Hurricane Sandy (no content based filter was applied).\n",
    "- Data format: comma-separated values (.csv) files containing the tweet ID, the time stamp of the tweet, a field indicating whether the tweet contains word \"sandy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Sandy ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports the list of 6 Million IDs\n",
    "data = pd.read_csv('../../release.txt',sep= ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag:search.twitter.com,2005:260244087901413376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088203403264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088161439744...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088819945472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tag:search.twitter.com,2005:260244089080004609...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  tag:search.twitter.com,2005:260244087901413376...\n",
       "1  tag:search.twitter.com,2005:260244088203403264...\n",
       "2  tag:search.twitter.com,2005:260244088161439744...\n",
       "3  tag:search.twitter.com,2005:260244088819945472...\n",
       "4  tag:search.twitter.com,2005:260244089080004609..."
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6554744, 1)"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into respective columns, create datetime column, drop unnecessary columns\n",
    "\n",
    "df = data[0].map(lambda x: x.split('\\t'))\n",
    "df = pd.DataFrame(df)\n",
    "df['timestamp'] = df[0].map(lambda x: x[1])\n",
    "df['tweet_id'] = df[0].map(lambda x: x[0])\n",
    "df['bool'] = df[0].map(lambda x: x[2])\n",
    "df = df.drop(columns=0)\n",
    "df['tweet_id'] = df['tweet_id'].map(lambda x: x.split(':')[2])\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.drop(columns=['timestamp','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260244088203403264</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id            datetime\n",
       "0  260244087901413376 2012-10-22 05:00:00\n",
       "1  260244088203403264 2012-10-22 05:00:00\n",
       "2  260244088161439744 2012-10-22 05:00:00\n",
       "3  260244088819945472 2012-10-22 05:00:00\n",
       "4  260244089080004609 2012-10-22 05:00:00"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ID List for Sandy Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to simulate the sort of access to Twitter that FEMA would have during a crisis situation, i.e.; all geotagged and timestamped tweets within some period of time.  Moreover, we anticipated that our classes in our final model would be unbalanced, because actually critical disaster tweets would be quite rare.  We wanted to sample from the period that would have as many of these as possible, in order to have more of them to train on and rely less on bootstrapping.\n",
    "\n",
    "Accordingly, we chose to sample from the window surrounding the landfall of Hurricane Sandy in New Jersey and New York (~8PM ET, October 29th, 2012.  We calculated that we could reasonably aim to pull about 180000 tweets for the training set, timewise.  We chose to pull all tweets from the list for the 3 hour period spanning from 1 hour prior to landfall to 2 hours afterward, so approximately 7PM-10PM that night. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftime = df.sort_values('datetime')\n",
    "dftime = dftime.reset_index(drop=True)\n",
    "\n",
    "#Picked the time index corresponding approximately to landfall of the hurricane \n",
    "dftime[dftime['datetime']=='2012-10-30 00:00:01'].head()\n",
    "\n",
    "#creates our major id list, from approximate time of landfall in NJ to about 3 hours later \n",
    "#(i.e., 180000 tweets down the timestamp-sorted ID list), all geotagged tweets in that time\n",
    "#continuous timespan also allows us to show complete minute to minute mapping visualization\n",
    "sandy_id_time = dftime.loc[4428365:4608434,:]\n",
    "sandy_id_time.to_csv('sandy_train_ids.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Tweets with Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of us created Twitter Development accounts and submitted applications for the project.  We each created two sets of [Twitter API](https://developer.twitter.com/en/apply-for-access) app keys so that we could pull tweets in tandem to allow for higher volume data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'INSERT KEY HERE'\n",
    "CONSUMER_SECRET = 'INSERT KEY HERE'\n",
    "\n",
    "OAUTH_TOKEN = 'INSERT KEY HERE'\n",
    "OAUTH_SECRET = 'INSERT KEY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a Single Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260244088161439744"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_train_ids['tweet_id'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Mon Oct 22 05:00:00 +0000 2012',\n",
       " 'id': 260244088161439744,\n",
       " 'id_str': '260244088161439744',\n",
       " 'text': '@NOT_savinHOES Not r yu upp',\n",
       " 'truncated': False,\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'screen_name': 'NOT_savinHOES',\n",
       "    'name': '01.18ü§∏üèΩ\\u200d‚ôÄÔ∏è',\n",
       "    'id': 293455555,\n",
       "    'id_str': '293455555',\n",
       "    'indices': [0, 14]}],\n",
       "  'urls': []},\n",
       " 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': 293455555,\n",
       " 'in_reply_to_user_id_str': '293455555',\n",
       " 'in_reply_to_screen_name': 'NOT_savinHOES',\n",
       " 'user': {'id': 401231570,\n",
       "  'id_str': '401231570',\n",
       "  'name': 'Jay ü§∑üèΩ\\u200d‚ôÇÔ∏è',\n",
       "  'screen_name': 'JayyLive202',\n",
       "  'location': 'Washington, DC, USA',\n",
       "  'description': '–∫Œπ–∏g  ùŒ±–º—î—ïüé©üèÜ 25. D[M]V | üëª:OfficialJaymes üì∏Insta:Oh.ThatsAlexx',\n",
       "  'url': 'https://t.co/a6Q76c6YNl',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/a6Q76c6YNl',\n",
       "      'expanded_url': 'https://link.dosh.cash/JAMESR117',\n",
       "      'display_url': 'link.dosh.cash/JAMESR117',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 803,\n",
       "  'friends_count': 576,\n",
       "  'listed_count': 0,\n",
       "  'created_at': 'Sun Oct 30 07:41:09 +0000 2011',\n",
       "  'favourites_count': 326,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 18359,\n",
       "  'lang': 'en',\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'C0DEED',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1070198512380461056/zthonwAC_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1070198512380461056/zthonwAC_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/401231570/1543990185',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': {'type': 'Point', 'coordinates': [40.2371544, -76.8206691]},\n",
       " 'coordinates': {'type': 'Point', 'coordinates': [-76.8206691, 40.2371544]},\n",
       " 'place': {'id': 'b8ce2948ffafff5f',\n",
       "  'url': 'https://api.twitter.com/1.1/geo/id/b8ce2948ffafff5f.json',\n",
       "  'place_type': 'city',\n",
       "  'name': 'Bressler-Enhaut-Oberlin',\n",
       "  'full_name': 'Bressler-Enhaut-Oberlin, PA',\n",
       "  'country_code': 'US',\n",
       "  'country': 'United States',\n",
       "  'contained_within': [],\n",
       "  'bounding_box': {'type': 'Polygon',\n",
       "   'coordinates': [[[-76.831479, 40.22417],\n",
       "     [-76.811937, 40.22417],\n",
       "     [-76.811937, 40.242082],\n",
       "     [-76.831479, 40.242082]]]},\n",
       "  'attributes': {}},\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 0,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.show_status(id='260244088161439744')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pull Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were aware the cap was 900 tweets in 15 minutes.  While we could set up a Twython call to run through some number of indices in our id list, once it hit the 900 limit, it would continue to mow through indices without actually getting anything.  This 900 count includes the significant percentage (~25%) of these old Sandy tweets that have since been deleted and yield no information, they are still counted as tweet calls.  So basically we would move 900 indices through the tweet id list per pull, regardless. \n",
    "\n",
    "We needed a way to automate looping through this pull process at least a few times so as not to end up with some ridiculous number of csvs, and so we could leave things running.  In order to achieve this, we needed the loop to know where to pick up on each new pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Goes through a block of 900 indices from the tweet list, from some start index.\n",
    "#Pulls tweet dictionary if tweet exists and adds to lst, otherwise continues to next index.\n",
    "#Returns a list with the start index for the next pull, and the lst containing all the tweet dicts.\n",
    "#Tqdm allows us to track the progress visually with each pull, as seen below.\n",
    "\n",
    "def tweet_pull(start_index):\n",
    "    tweet = None\n",
    "    lst = []\n",
    "    for i in tqdm(range(start_index,start_index+900)):\n",
    "        try:\n",
    "            dct = twitter.show_status(id=str(sandy_train_ids['tweet_id'][i]))\n",
    "            lst.append(dct)\n",
    "        except:\n",
    "            tweet = None\n",
    "        sandy_train_ids.set_value(i, 'tweet_texts', tweet)\n",
    "    return [start_index+900,lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]C:\\Users\\eamon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:19<00:00, 13.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:18<00:00, 11.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:25<00:00, 10.57it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:29<00:00, 10.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#While loop for tweet pulling\n",
    "#Simply set count = [index you want to start at], and set while count < [index you want to end at] (multiple of 900, ideally)\n",
    "#The while loop will run through all the indices in pulls of 900, shifting the start index up 900 each time,\n",
    "#and sleeping 15 minutes after each pull to make sure we are never drawing on empty.\n",
    "#The pulls are added to a single list of dictionaries that can be converted into a df.\n",
    "\n",
    "count = 0\n",
    "tweets = []\n",
    "while count < 3600:    \n",
    "    pull = tweet_pull(count)\n",
    "    tweets.extend(pull[1])\n",
    "    count = pull[0]\n",
    "    sleep(900) #15 minute limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>possibly_sensitive_appealable</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [{'text': 'ilovemaggiesmith', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': 'c55500e8cd2a1c64', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>\"I suppose she has an appropriate costume for ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 24753438, 'id_str': '24753438', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-76.8206691,...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.2371544, ...</td>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': 'b8ce2948ffafff5f', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@NOT_savinHOES Not r yu upp</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 401231570, 'id_str': '401231570', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-79.20266541...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [34.69318931,...</td>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '6057f1e35bcc6c20', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Hit and Run is so sad..</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 123368790, 'id_str': '123368790', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-71.04264063...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.44167162,...</td>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '75f5a403163f6f95', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Who's up?</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 47812293, 'id_str': '47812293', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.08961896...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.09464892,...</td>\n",
       "      <td>260244089985957888</td>\n",
       "      <td>260244089985957888</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '29aaa88d9fe74b50', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@augustushazel idk I'm just ugly or annoying o...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 274750107, 'id_str': '274750107', 'name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors                                        coordinates  \\\n",
       "0         None                                               None   \n",
       "1         None  {'type': 'Point', 'coordinates': [-76.8206691,...   \n",
       "2         None  {'type': 'Point', 'coordinates': [-79.20266541...   \n",
       "3         None  {'type': 'Point', 'coordinates': [-71.04264063...   \n",
       "4         None  {'type': 'Point', 'coordinates': [-80.08961896...   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Oct 22 05:00:00 +0000 2012   \n",
       "1  Mon Oct 22 05:00:00 +0000 2012   \n",
       "2  Mon Oct 22 05:00:00 +0000 2012   \n",
       "3  Mon Oct 22 05:00:00 +0000 2012   \n",
       "4  Mon Oct 22 05:00:00 +0000 2012   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  {'hashtags': [{'text': 'ilovemaggiesmith', 'in...               NaN   \n",
       "1  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "\n",
       "   favorite_count  favorited  \\\n",
       "0               0      False   \n",
       "1               0      False   \n",
       "2               0      False   \n",
       "3               0      False   \n",
       "4               0      False   \n",
       "\n",
       "                                                 geo                  id  \\\n",
       "0                                               None  260244087901413376   \n",
       "1  {'type': 'Point', 'coordinates': [40.2371544, ...  260244088161439744   \n",
       "2  {'type': 'Point', 'coordinates': [34.69318931,...  260244088819945472   \n",
       "3  {'type': 'Point', 'coordinates': [42.44167162,...  260244089080004609   \n",
       "4  {'type': 'Point', 'coordinates': [42.09464892,...  260244089985957888   \n",
       "\n",
       "               id_str                        ...                         lang  \\\n",
       "0  260244087901413376                        ...                           en   \n",
       "1  260244088161439744                        ...                           en   \n",
       "2  260244088819945472                        ...                           en   \n",
       "3  260244089080004609                        ...                           en   \n",
       "4  260244089985957888                        ...                           en   \n",
       "\n",
       "                                               place possibly_sensitive  \\\n",
       "0  {'id': 'c55500e8cd2a1c64', 'url': 'https://api...                NaN   \n",
       "1  {'id': 'b8ce2948ffafff5f', 'url': 'https://api...                NaN   \n",
       "2  {'id': '6057f1e35bcc6c20', 'url': 'https://api...                NaN   \n",
       "3  {'id': '75f5a403163f6f95', 'url': 'https://api...                NaN   \n",
       "4  {'id': '29aaa88d9fe74b50', 'url': 'https://api...                NaN   \n",
       "\n",
       "   possibly_sensitive_appealable retweet_count  retweeted  \\\n",
       "0                            NaN             0      False   \n",
       "1                            NaN             0      False   \n",
       "2                            NaN             0      False   \n",
       "3                            NaN             0      False   \n",
       "4                            NaN             0      False   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text truncated  \\\n",
       "0  \"I suppose she has an appropriate costume for ...     False   \n",
       "1                        @NOT_savinHOES Not r yu upp     False   \n",
       "2                            Hit and Run is so sad..     False   \n",
       "3                                          Who's up?     False   \n",
       "4  @augustushazel idk I'm just ugly or annoying o...     False   \n",
       "\n",
       "                                                user  \n",
       "0  {'id': 24753438, 'id_str': '24753438', 'name':...  \n",
       "1  {'id': 401231570, 'id_str': '401231570', 'name...  \n",
       "2  {'id': 123368790, 'id_str': '123368790', 'name...  \n",
       "3  {'id': 47812293, 'id_str': '47812293', 'name':...  \n",
       "4  {'id': 274750107, 'id_str': '274750107', 'name...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2569, 26)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy.to_csv('example_pull.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We coordinated to split up the job of pulling these loops.  We looped through a total of 180000 tweets from the landfall of Sandy.  We used Google Colab and Google Cloud Computing to run our pull loops over long stretches and collected data into a handful of csvs, which we combined to create our main data set for model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All the Pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('alexpulls.csv')\n",
    "data2 = pd.read_csv('eamonpulls.csv')\n",
    "data3 = pd.read_csv('javipulls.csv')\n",
    "\n",
    "data = pd.concat([data1,data2,data3], ignore_index=True)\n",
    "data.drop(columns=['Unnamed: 0'])\n",
    "data.dropna(subset=['id','text','created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_csv('sandy_landfall.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for Disaster Tweets: Figure Eight Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first issues we ran into in approaching our newly collected dataset is scale.  We knew we would have to manually label the critically relevant tweets, however it was unfeasible to search through over 100k tweets to do this.  So how could we whittle the larger body of tweets down to a selection of at least disaster-related tweets that we could then go through manually.\n",
    "\n",
    "We discovered a dataset on the website Figure Eight that had also been posted to Kaggle.  It was essentially a dataset of tweets from the time and location of recent disaster scenarios which had been labeled for whether they referred to the actual disaster or not.  So we figured that by training on this dataset first, we could develop a way to whittle our list down to at least disaster-related tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Kaggle Data (remove http addresses with regex, lowercase the text)\n",
    "df = pd.read_csv('figure_eight_dataset.csv')\n",
    "df['text'] = df['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "df['text'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "df = df[df.duplicated('text')==False]\n",
    "df['text']= df['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , stratify=y, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran Logistic Regression GridSearches with multiple vectorizers (Tdidf, Hashing, Count), and found that CountVectorizer performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2, 4, 6],                            #In the paragraph after I will explain the parameters chosen\n",
    "    'vect__ngram_range':[(1,2),(1,3)],\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'model__penalty':['l1','l2'],  \n",
    "    'model__C':[0.01, 0.1 ,1]   \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', RandomForestClassifier() )\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'vect__ngram_range':[(1,2),(1,3)],\n",
    "    'model__n_estimators':[75, 200, 500],\n",
    "    'model__max_depth':[5, 25, 75],\n",
    "    'model__min_samples_split':[2,3,4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', svm.SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__kernel':['rbf','poly'],\n",
    "    'model__C':[.1, 1, 10]  \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[1,2,4, 6],\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'model__alpha': [0.1,1,10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran a number of different boosting classifiers with multiple vectorizers, of which XG Boost performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__n_estimators': [700, 1500],\n",
    "    'model__min_samples_split':[2,4,6],\n",
    "    'model__max_depth':[3,5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier with best LogReg, XGBoost, RandomForest (Final Model Choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we went with a VotingClassifier that combined the predictive input of multiple models (LogReg, XGBoost, and Random Forest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "        ('count_vect', CountVectorizer(min_df=2,  \n",
    "                                  ngram_range=(1, 3))),     \n",
    "        ('clf', VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                             (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                             #(\"pip3\", svm.SVC(kernel='rbf',C=10,probability=True)),\n",
    "                                             #(\"pip4\", MultinomialNB(alpha=1)), \n",
    "\n",
    "                                             (\"pip5\", RandomForestClassifier(max_depth=75, \n",
    "                                                                             min_samples_split=4, \n",
    "                                                                             n_estimators=200))],voting='soft'))\n",
    "         ])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross-val score for this model was 0.802, against a baseline accuracy of about 0.58.  This is the best result we saw in this modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the VotingClassifier on the Figure Eight Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, ngram_range = (1,3))\n",
    "\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft')\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "#\n",
    "print('Best Estimator Score Train: ', model.score(X_train_features, y_train))\n",
    "print('Best Estimator Score Test: ', model.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_features)\n",
    "\n",
    "def make_nice_conmat(y_test, preds):\n",
    "\n",
    "    cmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, preds)}')\n",
    "    print(classification_report(y_test, preds))\n",
    "    return pd.DataFrame(cmat, columns=['Predicted ' + str(i) for i in ['Regular Tweets','Disaster Tweets']],\\\n",
    "            index=['Actual ' + str(i) for i in ['Regular Tweets','Disaster Tweets']])\n",
    "\n",
    "make_nice_conmat(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/OdETYZT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VotingClassifier on Entire Figure 8 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=2, ngram_range = (1,3))\n",
    "\n",
    "X_features = vectorizer2.fit_transform(X)\n",
    "\n",
    "\n",
    "model2 = VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft')\n",
    "model2.fit(X_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X_features,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(X_features)\n",
    "df['predictions'] = predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for Disaster Tweets: Crisis Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were less than completely satisfied with the output of the Kaggle model, and in particular, looking closely at the dataset, felt that some of the labeling was suspect or flat out incorrect.  We had to time-box ourselves to a degree in this project, but we decided that a keyword list could be useful in further filtering for tweets of disaster relevance.  We felt it was better to cast a wider net, as we would be loathe to miss a truly critical tweet.\n",
    "\n",
    "We were able to locate a long, standardized list of disaster-related keywords on CrisisNLP.org.  We imported this and added some words of our own.  In particular, the CrisisNLP list appeared to be designed for circumstances in which the words are pre-tokenized.  Seeing as we could filter based on the complete tweet texts, we condensed the list somewhat to avoid redundancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we added words of our own to the single word items in the existing CrisisNLP list.  \n",
    "#This is the list we will use to filter.  An enterprising user of our model could edit this list themselves very easily:\n",
    "keys_slist = [\n",
    " '911',\n",
    " 'affected',\n",
    " 'aftermath',\n",
    " 'ambulance',\n",
    " 'arrest',\n",
    " 'attack',\n",
    " 'authorities',\n",
    " 'blast',\n",
    " 'blood',\n",
    " 'body',\n",
    " 'bodies',\n",
    " 'bomber',\n",
    " 'bombing',\n",
    " 'braces',\n",
    " 'buried',\n",
    " 'burn',\n",
    " 'casualties',\n",
    " 'cleanup',\n",
    " 'collapse',\n",
    " 'collapsed',\n",
    " 'conditions',\n",
    " 'crash',\n",
    " 'crisis',\n",
    " 'damage',\n",
    " 'danger',\n",
    " 'dead',\n",
    " 'deadly',\n",
    " 'death',\n",
    " 'destroyed',\n",
    " 'destruction',\n",
    " 'devastating',\n",
    " 'disaster',\n",
    " 'displaced',\n",
    " 'donate',\n",
    " 'dozens',\n",
    " 'dramatic',\n",
    " 'drown',\n",
    " 'emergency',\n",
    " 'enforcement',\n",
    " 'evacu',\n",
    " 'events',\n",
    " 'explosion',\n",
    " 'fallen',\n",
    " 'fatalities',\n",
    " 'fire',\n",
    " 'flood',\n",
    " 'flooding',\n",
    " 'floodwaters',\n",
    " 'footage',\n",
    " 'gun',\n",
    " 'help!',\n",
    " 'hurricane',\n",
    " 'imminent',\n",
    " 'impacted',\n",
    " 'injured',\n",
    " 'injuries',\n",
    " 'inundated',\n",
    " 'investigation',\n",
    " 'killed',\n",
    " 'landfall',\n",
    " 'levy',\n",
    " 'looting',\n",
    " 'magnitude',\n",
    " 'massive',\n",
    " 'military',\n",
    " 'missing',\n",
    " 'nursing',\n",
    " 'outage',\n",
    " 'paramedic',\n",
    " 'prayers',\n",
    " 'praying',\n",
    " 'ravaged',\n",
    " 'recede',\n",
    " 'recover',\n",
    " 'redcross',\n",
    " 'relief',\n",
    " 'rescue',\n",
    " 'rescuers',\n",
    " 'residents',\n",
    " 'responders',\n",
    " 'rubble',\n",
    " 'saddened',\n",
    " 'safety',\n",
    " 'scream',\n",
    " 'seismic',\n",
    " 'seizure',\n",
    " 'shelter',\n",
    " 'shooter',\n",
    " 'shooting',\n",
    " 'shot',\n",
    " 'soldier',\n",
    " 'storm',\n",
    " 'stream',\n",
    " 'surviving',\n",
    " 'survivor',\n",
    " 'terrifying',\n",
    " 'terror',\n",
    " 'toll',\n",
    " 'tornado',\n",
    " 'torrential',\n",
    " 'toxins',\n",
    " 'tragedy',\n",
    " 'tragic',\n",
    " 'troops',\n",
    " 'twister',\n",
    " 'unaccounted',\n",
    " 'urgent',\n",
    " 'victims',\n",
    " 'volunteers',\n",
    " 'warning',\n",
    " 'wounded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Filters to Generate List of Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Figure 8 Model to Sandy Landfall Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sandy_landfall.csv')\n",
    "\n",
    "#Cleaning\n",
    "data.dropna(subset=['id','text','created_at'], inplace=True)\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "data['text'] = data['text'].map(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "data['text'] = data['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "data = data[data.duplicated('text')==False]\n",
    "\n",
    "#Apply VotingClassifier model to data\n",
    "with open('kaggle_model_2.pkl', 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "data['predicted']= model.predict(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Keyword Filter to Sandy Landfall Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolates tweets that have not been predicted as disaster tweets by the Figure 8 model\n",
    "non_predicted = data[data['predicted']==0]\n",
    "predicted = data[data['predicted']==1]\n",
    "\n",
    "#Recall that keys_slist is the list of crisis keywords defined explicitly above\n",
    "#This maps through the tweets not labeled as disaster and labels them as disaster if they include keywords\n",
    "non_predicted['predicted'] = non_predicted['text'].map(lambda x: 1 if sum([x.find(i) + 1 for i in keys_slist])>0 else 0)\n",
    "\n",
    "#Combines keyword flags and Figure 8 model flags to produce the set of all disaster tweets\n",
    "keywords =non_predicted[non_predicted['predicted']==1]\n",
    "disaster_tweets = pd.concat([keywords,predicted], ignore_index=True)\n",
    "\n",
    "#Produces set of all regular tweets\n",
    "regular_tweets = non_predicted[non_predicted['predicted']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets.to_csv('disaster_tweets.csv')\n",
    "regular_tweets.to_csv('regular_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Identifying Critical Tweets Among Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Labeling Disaster Related Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combed through the approximately 9000 tweets identified by the Figure Eight Model and Keyword Filtering as disaster tweets. We manually identified about 900 tweets that we felt met criteria for \"critical\" - i.e.; novel information that could be immediately relevant to emergency responders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload disaster tweets with labels now included\n",
    "disaster_labeled = pd.read_csv('./manual_tags_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263097193563566080</td>\n",
       "      <td>rt @passantino: wow: floodwaters inundate grou...</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.7245571,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.025018, -...</td>\n",
       "      <td>{'id': 'de599025180e2ee7', 'url': 'https://api...</td>\n",
       "      <td>{'id': 373792493, 'id_str': '373792493', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263097199213285377</td>\n",
       "      <td>some folks maybe feelin lonely bein in a storm...</td>\n",
       "      <td>Tue Oct 30 01:57:15 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-76.8325555,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [38.89184242,...</td>\n",
       "      <td>{'id': '19f2fcdf0d209467', 'url': 'https://api...</td>\n",
       "      <td>{'id': 250905822, 'id_str': '250905822', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263097201469845504</td>\n",
       "      <td>#itjustgotreal ... #iphonealerts  @ #hurricane...</td>\n",
       "      <td>Tue Oct 30 01:57:15 +0000 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'b6ea2e341ba4356f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 112052977, 'id_str': '112052977', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'itJustGotReal', 'indic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>und</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263097206125518849</td>\n",
       "      <td>@ahurricanesandy hey #sandy get your ass down ...</td>\n",
       "      <td>Tue Oct 30 01:57:16 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-79.99661002...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [35.97201904,...</td>\n",
       "      <td>{'id': 'aef8c3da277ca498', 'url': 'https://api...</td>\n",
       "      <td>{'id': 266676966, 'id_str': '266676966', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'Sandy', 'indices': [21...</td>\n",
       "      <td>364217289.0</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263097208923099136</td>\n",
       "      <td>everytime the wind picks up it sounds like som...</td>\n",
       "      <td>Tue Oct 30 01:57:17 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.4487703,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [37.213948, -...</td>\n",
       "      <td>{'id': '820684853e0f1eb6', 'url': 'https://api...</td>\n",
       "      <td>{'id': 479075523, 'id_str': '479075523', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'hurricanessuck', 'indi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263097193563566080  rt @passantino: wow: floodwaters inundate grou...   \n",
       "1  263097199213285377  some folks maybe feelin lonely bein in a storm...   \n",
       "2  263097201469845504  #itjustgotreal ... #iphonealerts  @ #hurricane...   \n",
       "3  263097206125518849  @ahurricanesandy hey #sandy get your ass down ...   \n",
       "4  263097208923099136  everytime the wind picks up it sounds like som...   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Oct 30 01:57:13 +0000 2012   \n",
       "1  Tue Oct 30 01:57:15 +0000 2012   \n",
       "2  Tue Oct 30 01:57:15 +0000 2012   \n",
       "3  Tue Oct 30 01:57:16 +0000 2012   \n",
       "4  Tue Oct 30 01:57:17 +0000 2012   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0  {'type': 'Point', 'coordinates': [-80.7245571,...   \n",
       "1  {'type': 'Point', 'coordinates': [-76.8325555,...   \n",
       "2                                                NaN   \n",
       "3  {'type': 'Point', 'coordinates': [-79.99661002...   \n",
       "4  {'type': 'Point', 'coordinates': [-80.4487703,...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0  {'type': 'Point', 'coordinates': [41.025018, -...   \n",
       "1  {'type': 'Point', 'coordinates': [38.89184242,...   \n",
       "2                                                NaN   \n",
       "3  {'type': 'Point', 'coordinates': [35.97201904,...   \n",
       "4  {'type': 'Point', 'coordinates': [37.213948, -...   \n",
       "\n",
       "                                               place  \\\n",
       "0  {'id': 'de599025180e2ee7', 'url': 'https://api...   \n",
       "1  {'id': '19f2fcdf0d209467', 'url': 'https://api...   \n",
       "2  {'id': 'b6ea2e341ba4356f', 'url': 'https://api...   \n",
       "3  {'id': 'aef8c3da277ca498', 'url': 'https://api...   \n",
       "4  {'id': '820684853e0f1eb6', 'url': 'https://api...   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'id': 373792493, 'id_str': '373792493', 'name...   \n",
       "1  {'id': 250905822, 'id_str': '250905822', 'name...   \n",
       "2  {'id': 112052977, 'id_str': '112052977', 'name...   \n",
       "3  {'id': 266676966, 'id_str': '266676966', 'name...   \n",
       "4  {'id': 479075523, 'id_str': '479075523', 'name...   \n",
       "\n",
       "                                            entities  in_reply_to_user_id  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "1  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "2  {'hashtags': [{'text': 'itJustGotReal', 'indic...                  NaN   \n",
       "3  {'hashtags': [{'text': 'Sandy', 'indices': [21...          364217289.0   \n",
       "4  {'hashtags': [{'text': 'hurricanessuck', 'indi...                  NaN   \n",
       "\n",
       "  lang  predicted  tag  \n",
       "0   en        1.0  1.0  \n",
       "1   en        1.0  0.0  \n",
       "2  und        1.0  0.0  \n",
       "3   en        1.0  0.0  \n",
       "4   en        1.0  0.0  "
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tag** column indicates whether the tweet was manually tagged as \"critical\".  This is based on our first tagging run, in which we cast a relatively wide net.  The **predicted** column indicates whether the tweet is disaster related (all of these were, of course), in case we want to concatenate with our regular tweets data (predicted = 0) later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is also a good look at what our tweet data dictionaries contain.  Not everything is immediately relevant to the project here, but we left in anything that might be used predictively later on.  Obviously, we have the specific ID that we can use to look up the tweet online, or use as an index.  We also have:\n",
    "\n",
    "- the text of the tweet\n",
    "- the timestamp\n",
    "- the geo-coordinate information\n",
    "- a place dictionary that contains information about the area including the city/neighborhood of origin\n",
    "- a dictionary of information about the user sending the tweet\n",
    "- a dictionary that grabs any hashtags the tweet contains\n",
    "- a column that allows us to tell if the tweet was a reply or not\n",
    "- the language of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Disaster Tweets - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>263097391169826817</td>\n",
       "      <td>@rwzombie fuck #hurricanesandy keep voting</td>\n",
       "      <td>Tue Oct 30 01:58:00 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-74.95308309...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.04676075,...</td>\n",
       "      <td>{'id': '31fbce652077706d', 'url': 'https://api...</td>\n",
       "      <td>{'id': 25303398, 'id_str': '25303398', 'name':...</td>\n",
       "      <td>{'hashtags': [{'text': 'hurricanesandy', 'indi...</td>\n",
       "      <td>43469093.0</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                        text  \\\n",
       "30  263097391169826817  @rwzombie fuck #hurricanesandy keep voting   \n",
       "\n",
       "                        created_at  \\\n",
       "30  Tue Oct 30 01:58:00 +0000 2012   \n",
       "\n",
       "                                          coordinates  \\\n",
       "30  {'type': 'Point', 'coordinates': [-74.95308309...   \n",
       "\n",
       "                                                  geo  \\\n",
       "30  {'type': 'Point', 'coordinates': [40.04676075,...   \n",
       "\n",
       "                                                place  \\\n",
       "30  {'id': '31fbce652077706d', 'url': 'https://api...   \n",
       "\n",
       "                                                 user  \\\n",
       "30  {'id': 25303398, 'id_str': '25303398', 'name':...   \n",
       "\n",
       "                                             entities  in_reply_to_user_id  \\\n",
       "30  {'hashtags': [{'text': 'hurricanesandy', 'indi...           43469093.0   \n",
       "\n",
       "   lang  predicted  tag  \n",
       "30   en        1.0  NaN  "
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if we missed or deleted any cells while tagging manually\n",
    "disaster_labeled[disaster_labeled['tag'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    8694\n",
       "1.0     836\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled['tag'] = disaster_labeled['tag'].fillna(0)\n",
    "disaster_labeled['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled.dropna(subset=['id','text','created_at'],inplace=True)\n",
    "\n",
    "#english language only\n",
    "disaster_labeled = disaster_labeled[disaster_labeled['lang']=='en']\n",
    "#create readable datetime column and sort by datetime\n",
    "disaster_labeled['datetime'] = pd.to_datetime(disaster_labeled['created_at'])\n",
    "disaster_labeled = disaster_labeled.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "disaster_labeled = disaster_labeled[['id','text','datetime','geo','predicted','tag']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "disaster_labeled['text'] = disaster_labeled['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "disaster_labeled.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "disaster_labeled['text'] = disaster_labeled['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "disaster_labeled.dropna(subset=['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>geo</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>@godseyg i was arrested about 36 hours later. ...</td>\n",
       "      <td>2012-10-30 00:00:02</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12081393,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>wish i was wiff my love during this disaster #...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.28866479,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>im a hurricane vet.. so #hurricanesandy isnt a...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12088316,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>the scorpions - rock you like a hurricane #201...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.60095122,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>lights out @ frankenstorm apocalypse - hurrica...</td>\n",
       "      <td>2012-10-30 00:00:06</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.79093941,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  2.630677e+17  @godseyg i was arrested about 36 hours later. ...   \n",
       "1  2.630677e+17  wish i was wiff my love during this disaster #...   \n",
       "2  2.630677e+17  im a hurricane vet.. so #hurricanesandy isnt a...   \n",
       "3  2.630677e+17  the scorpions - rock you like a hurricane #201...   \n",
       "4  2.630677e+17  lights out @ frankenstorm apocalypse - hurrica...   \n",
       "\n",
       "              datetime                                                geo  \\\n",
       "0  2012-10-30 00:00:02  {'type': 'Point', 'coordinates': [39.12081393,...   \n",
       "1  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [41.28866479,...   \n",
       "2  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [39.12088316,...   \n",
       "3  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [42.60095122,...   \n",
       "4  2012-10-30 00:00:06  {'type': 'Point', 'coordinates': [40.79093941,...   \n",
       "\n",
       "   predicted  tag  \n",
       "0        1.0  0.0  \n",
       "1        1.0  0.0  \n",
       "2        1.0  0.0  \n",
       "3        1.0  0.0  \n",
       "4        1.0  0.0  "
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled.to_csv('./disaster_labeled.csv',index=False)\n",
    "\n",
    "disaster_labeled[disaster_labeled['tag']==1].to_csv('./critical.csv', index=False)\n",
    "disaster_labeled[disaster_labeled['tag']==0].to_csv('./disaster_nonrel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting and Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually tagging, we had about a 9:1 ratio of disaster non critical to disaster critical tweets, so baseline accuracy of about 90%.  In order to resolve this it was necessary to oversample the tweets we had labeled critical (bootstrapping).  We expanded the 900ish critical tweets to 9000, balancing the classes evenly.\n",
    "\n",
    "While manually tagging, we had often felt that some of the tweets we labeled critical were borderline, while others were immediate and dire, and highly useful potentially.  We felt this should be reflected in our bootstrapping by using weighted probabilities in our oversampling.  We thus went through the 900 critical tweets and weighted them 1, 3, 5, or 10 based on their degree of relevance to emergency personnel who might be scanning twitter for information.  We then normalized these to percentages of 1 to create probabilities for use in bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = pd.read_csv('weighted_critical.csv')\n",
    "\n",
    "#Converts manually assigned weights to bootstrap weighted probabilities\n",
    "weighted['weight']= weighted['weight'].map(lambda x: x/weighted['weight'].sum())\n",
    "\n",
    "boot = weighted.sample(9000,replace=True, weights=weighted['weight'])\n",
    "boot.drop(columns = ['weight'],inplace=True)\n",
    "\n",
    "nonrel = pd.read_csv('disaster_nonrel.csv')\n",
    "nonrel.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "disaster = pd.concat([nonrel, boot], ignore_index=True)\n",
    "#So now we have a training set composed of roughly equal classes - half are irrelevant disaster-related tweets, \n",
    "#and the other half are critical tweets that have been bootstrapped with weights to appear multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model to Predict Actual Relevance from within Disaster Related Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried our hand with a few different things, but ended up settling on a similarly trained VotingClassifier to our earlier Figure 8 Model, with combination of trained LogReg, XGBoost and Random Forest Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = disaster['text']\n",
    "y = disaster['tag']\n",
    "\n",
    "critical_from_disaster_model = Pipeline([\n",
    "        ('count_vect', CountVectorizer(min_df=2,  \n",
    "                                  ngram_range=(1, 3))),     \n",
    "        ('clf', VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft'))\n",
    "         ])\n",
    "critical_from_disaster_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed cross-validation scores in the mid to high 90s with this training process, however this was obviously the result of highly weighted bootstrapped replacement tweets appearing in the test split as well as in the train split.  The only way to really assess performance is to run the model on test data and see how its predictions perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud: Words that Help Identify Critical Tweets Among Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/FL6iEue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously the words here are somewhat specific to the Sandy Hurricane, which we would expect.  Continuing to train with a wider variety of disasters (wildfires, tornadoes, mass shootings, earthquakes, etc - even everyday emergencies perhaps) would allow the model to become more generalized.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2-Phase Model {1. Disaster Filter, 2. Predict Critical} on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ID List for Sandy Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our test set, we wanted to sample from a wider section of the hurricane's course.  We aimed for about 40000 tweets for the test set.  We decided to randomly select 40000 tweets from the 2 million or so post-landfall time window tweets, so, starting at the end of the window of our training set (10PM on 10/29) through the last timestamp in the ID List (11/2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates our test set id list\n",
    "#Random selection of tweets over the rest of the hurricane, so we can show geographic progression in mapping as well\n",
    "#40,500 was chosen as n because it is a convenient multiple of 2700 (900*3) for the tweet pulls\n",
    "\n",
    "sandy_random = dftime.loc[4608435:6000000,:]\n",
    "sandy_random = sandy_random.sample(40500,replace=False,random_state=37)\n",
    "sandy_random = sandy_random.sort_values('datetime')\n",
    "sandy_random = sandy_random.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_random.to_csv('sandy_random.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the main set, we ran tweet pulls in tandem and combined the resulting csvs to complete the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('./alexrandom1.csv')\n",
    "test2 = pd.read_csv('./alexrandom2.csv')\n",
    "test3 = pd.read_csv('./eamonrandom1.csv')\n",
    "test4 = pd.read_csv('./eamonrandom2.csv')\n",
    "test5 = pd.read_csv('./eamonrandom3.csv')\n",
    "test6 = pd.read_csv('./javirandom1.csv')\n",
    "test7 = pd.read_csv('./javirandom2.csv')\n",
    "test8 = pd.read_csv('./javirandom3.csv')\n",
    "test9 = pd.read_csv('./javirandom4.csv')\n",
    "\n",
    "testset = pd.concat([test1,test2,test3,test4,test5,test6,test7,test8,test9],ignore_index=True)\n",
    "\n",
    "testset.dropna(subset=['id','text','created_at'],inplace=True)\n",
    "\n",
    "testset = testset[testset['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "testset['datetime'] = pd.to_datetime(testset['created_at'])\n",
    "testset = testset.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#english language only\n",
    "testset = testset[testset['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "testset['datetime'] = pd.to_datetime(testset['created_at'])\n",
    "testset = testset.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "testset = testset[['id','text','datetime','geo']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "testset['text'] = testset['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "testset.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "testset['text'] = testset['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "testset.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "testset['id'] = testset['id'].map(lambda x: int(x))\n",
    "\n",
    "testset['text'] = testset['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "testset['text']= testset['text'].map(lambda x: x.lower())\n",
    "testset['text'] = testset['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "testset['text'] = testset['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "testset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Filter/Models on the Test Data to Identify Critical Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Figure 8 model and separate into regular and disaster tweets.\n",
    "testset['is_disaster']= kaggle_model.predict(testset['text'])\n",
    "test_regular = testset[testset['is_disaster']==0]\n",
    "test_disaster = testset[testset['is_disaster']==1]\n",
    "\n",
    "#Apply keyword filter to tweets identified as regular by Figure 8\n",
    "#to identify additional disaster tweets, and combine to collect all disaster tweets\n",
    "test_regular['is_disaster'] = test_regular['text'].map(lambda x: 1 if sum([x.find(i) + 1 for i in keys_slist])>0 else 0)\n",
    "contains_keywords = test_regular[test_regular['is_disaster']==1]\n",
    "test_disaster = pd.concat([contains_keywords,test_disaster], ignore_index=True)\n",
    "\n",
    "#Run Critical Tweet Identifier model on disaster tweets\n",
    "test_disaster['is_critical']= critical_from_disaster_model.predict(test_disaster['text'])\n",
    "\n",
    "#Save all regular tweets (not disaster related)\n",
    "test_regular.to_csv('test_regular.csv',index=False)\n",
    "\n",
    "#Save all disaster-related tweets that are not relevant\n",
    "test_disaster_nonrel = test_disaster[test_disaster['is_critical'] == 0]\n",
    "test_disaster_nonrel.to_csv('test_disaster_nonrel.csv',index=False)\n",
    "\n",
    "#Save all tweets that were identified as disaster AND critical\n",
    "test_critical = test_disaster[test_disaster['is_critical'] == 1]\n",
    "test_critical.to_csv('test_critical.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of the 25000 or so tweets in the test set, our two-phase model identified about 50 as potentially critical tweets.  You can review these below.  Given the limitations we had to work with in terms of bootstrapping, and simply not having that much data for the problem, we were quite pleased with the performance here.  Most of the tweets identified as critical do seem to correspond to situations where the hurricane is actively creating immediate problems.\n",
    "\n",
    "Comparing this to a selection of the non-relevant disaster tweets, or the regular tweets (seen below), we can see that the model is actually doing a pretty excellent job of homing in on real issues and ignoring other mentions of the storm.  This is a pretty remarkable result, and we have reason to expect that there is still a lot of potential for the model to improve, with an appropriately large and diverse data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets That Were Identified as Critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@wwegames @wwe @cmpunk step outside my house and swim in the flood\n",
      "----------\n",
      "the streets could be flooded and houses could be floating and coach would be like 5:30am practice on main street dress warm\n",
      "----------\n",
      "everywhere is flooded\n",
      "----------\n",
      "awesome my street is flooded..\n",
      "----------\n",
      "im alive no damage either a tree across the street broke. we have power\n",
      "----------\n",
      "the remains of the tree that fell on tracks by stony brook yesterday, was mostly dead already. \n",
      "----------\n",
      "no tree damage this morning @ mcdevitt field. the field is ok, just several leaves amp small branches down. \n",
      "----------\n",
      "@primetime_lerch @realfrankgeib24 @abegotwheels @a_stump_ swimming through the flooded half of red tail\n",
      "----------\n",
      "dragon #sandy ripped off roof #alexandria apt. building so loud, child thought it was a dragon no injuries @wusa9 \n",
      "----------\n",
      " rt @dopeasjordan: a wire just exploded outside on my block and electrocuted and set a dog on fire, w o w.\n",
      "----------\n",
      "woke up w no power just candles on couldnt text or make calls or use data lights out everywhere trees down felt like i was in #walkingdead\n",
      "----------\n",
      "creek road and 926 flooded #wcugis \n",
      "----------\n",
      "flooded fdr #sandy #frankenstorm @bsheridan  @ fdr drive \n",
      "----------\n",
      "whats left of our cherry tree... #hurricanesandy #picfx @ casa de urena \n",
      "----------\n",
      "my next door neighbors tree is threatening to kill our electricity #hurricanesandy \n",
      "----------\n",
      "welcome to l-town #hurricanesandy #boat #in #a #tree  @ water logged l-town \n",
      "----------\n",
      "fire on my street. geeze. probably the wesley kids. ahaha. #justassuming\n",
      "----------\n",
      "wait jones beach theater is flooded, it better be ready for one direction this summer \n",
      "----------\n",
      "tree down on carroll st. right in front of where that brownstone collapsed earlier this year. \n",
      "----------\n",
      "drove around town to see the damage amp where all these power company trucks are its bad up here. power company gets de \n",
      "----------\n",
      "520 steps to my powerless 26th floor apartment, just 12 steps to my powerful recover @ 46th street clubhouse \n",
      "----------\n",
      "@joshelliottabc live across the street from hospital 1st ave and 33rd flooded\n",
      "----------\n",
      "tree on house after sandy. no damage inside. thank god. \n",
      "----------\n",
      "lots of trees down in prospect park #sandy  @ prospect park west \n",
      "----------\n",
      "@fdny: qns 6-6 breezy point fire, 50 homes completely destroyed by fire ouch....\n",
      "----------\n",
      "front of building destroyed by sandy: a chelsea building near the corner of 8th amp w. 14th st. partially... \n",
      "----------\n",
      "@shawnmichaels 2 straight days off, 2 confirmed deaths in baltimore, md. 200,000 no power. #222 #sandy\n",
      "----------\n",
      "still no power at hemlock and spruce st in north andover, #nationalgrid #sandy\n",
      "----------\n",
      "river flooding 71st street @ bobby wagner walk \n",
      "----------\n",
      "access to hoboken from jersey city next to impossible. grove street. marin blvd., jersey ave flooded.\n",
      "----------\n",
      "in the movie 2012 nyc floods... nyc is flooded right now #freaky\n",
      "----------\n",
      "@severnaprkpatch tree hit house in berrywood south \n",
      "----------\n",
      "@jzanetti25 a tree fell on the house and killed two kids\n",
      "----------\n",
      "destruction around coned, the water actually moved cars around in the street. @ coned 750 east 16 street \n",
      "----------\n",
      "flood warning issued for glen burnie, md \n",
      "----------\n",
      "trees down in central park. #sandy  @ rustic overlook \n",
      "----------\n",
      "#sandy update: no power, cell, or landline service in huntington station, #longisland since 4pm yesterday. trees partially blocking roads\n",
      "----------\n",
      "tree blocking street in wantagh, ny #sandy \n",
      "----------\n",
      "tree in the middle of pierce rd #sandy2012  @ pierce rd \n",
      "----------\n",
      "@sandrabookman7 heres a tree down shot, taken at murdock n neried \n",
      "----------\n",
      "major tree damage in theodore roosevelt park. #ues #sandynyc  @ theodore roosevelt park museum park \n",
      "----------\n",
      "bloomberg: zone a evacuation order is in effect until all clear from building inspectors, which has not happened yet.\n",
      "----------\n",
      "@newzphotos grant rd tree on a car also...three calls\n",
      "----------\n",
      "downed trees across lubber street at quaker path in stony brook, n.y. @ quaker path \n",
      "----------\n",
      "just passed a massive tree down on the grounds of the museum of natural history. audible gasp from the unusually quiet bus passengers.\n",
      "#nyc\n",
      "----------\n",
      "water street flooded :/ hurricane sandy can suck it : #hurricane #hurricanesandy #water #s @ long warf park \n",
      "----------\n",
      "stay away from east avenue and wall street truck just took out all power line\n",
      "----------\n",
      "closed dirty alley or street request at 2217 etting st baltimore \n",
      "----------\n",
      "in numbers: 27,373 @coned customers in chelsea without power 96,257 in cooper square 21,281 in kips bay, for starters. #sandy\n",
      "----------\n",
      "#wellington closed due to flooding on rt 18 both eb/wb between sr 58 and oh-301 #traffic \n",
      "----------\n",
      "#cuyahoga closed due to flooding on w 150th st north of i 480 #traffic \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in test_critical['text']:\n",
    "    print(i)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets That Were Identified as Disaster Related but Not Critical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on a day when i am afraid, i will trust you, god -- psalm 56:3 pray for those enduring #hurricane #sandy\n",
      "----------\n",
      "fuck #hurricanesandy\n",
      "----------\n",
      "... just spent 2 hours chillin in my car. now im back in the house praying my phone lasts me the whole night lmao i hate not having power\n",
      "----------\n",
      "@kmbutler4 like the wizard of oz lol i really meant kansas remember the tornado and the flying monkeys lol\n",
      "----------\n",
      "@annanicolexxo i will when she gets back from the tests. im having a mini orgasm because im in an emergency room gt.lt\n",
      "----------\n",
      "@verizonwireless boy you guys are gonna hear it tomorrow when this hurricane is over...\n",
      "----------\n",
      "@steve_whosoever @joshuanason hi steve, joshua is from ri, im from pa. im sure youve been through a lot of hurricanes living on the gulf\n",
      "----------\n",
      "@allen_strk: really excited to see how the teams play out for survivor series. they suck\n",
      "----------\n",
      "the real risks from hurricane #sandy: \n",
      "----------\n",
      "so im without electricity and very limited service. a prayer to everyone who is being affected or has been affected badly.\n",
      "----------\n",
      "i dont know whats worse this damn storm or my brothers snoring #fml\n",
      "----------\n",
      "@msoblount: @princephillyock working in the dark at arcadia university ...persevering through the hurricane . love my son \n",
      "----------\n",
      "this one time, at hurricane camp, i stuck a package of ramen noodles in \n",
      "my vagina #fucksandy #survival\n",
      "----------\n",
      "getting thru the blackout #hurricanesandy caused here in #nyc  with the help of @arminvanbuurens @djmagazine 2012 awards set #trancefamily\n",
      "----------\n",
      "power outages: one more pragmatic reason to be #vegan. #lessfunkyfridgeforthewin #sandy\n",
      "----------\n",
      "@dailydeadnews evil dead remake looks intense. im excited bout carrie. but thoughts on lords of salem trailer was, ehhhh...\n",
      "----------\n",
      "@aaronwanat yes its a hurricane that hasnt really affected western ny but the bigger cities\n",
      "----------\n",
      "hoodlums #troublemakers #introuble #hurricanesandy  @ weldin hall \n",
      "----------\n",
      "praying for everyone at #nyu hospital. #sandy\n",
      "----------\n",
      "uff, so much bad news. i feel like were sitting pretty while the rest of the city burns/floods. you can do it, new york\n",
      "----------\n",
      "next hurricane should be called hurricane taylor sounds cute : hehe\n",
      "----------\n",
      "#NAME?\n",
      "----------\n",
      "@madmaxjr_ man stfu with that dead shit\n",
      "----------\n",
      "want to sleep with my window open, but the wind is going crazy #hurricaneproblems\n",
      "----------\n",
      "through this freak storm i still find something to make me relaxed and smile @jcovv @kpod23 see you two thanksgiving  \n",
      "----------\n",
      "this big bad hurricane had better at least knock out the power at rio, because im not feeling class tomorrow #dontlikethecold\n",
      "----------\n",
      "@kasiyan93 its not that would be a tragedy calllin it tho\n",
      "----------\n",
      "thanks for all the well wishes, plz know i am in a low risk area and many around me could use prayers more. lt3 @andrewneylon @ericneedleman\n",
      "----------\n",
      "dear hurricane sandy, \n",
      "can you take obama with you \n",
      "sincerely, smart americans\n",
      "----------\n",
      "waiting for the power to go out. @ frankenstorm apocalypse - hurricane sandy w/ 404 others \n",
      "----------\n",
      "@nbfirefighter90 your such a tard \n",
      "----------\n",
      "you will never find new yorks times square so empty -ever #hurricane sandy# at times square pic  \n",
      "----------\n",
      "pokemon song/britney sing-a-long, apples to apples, and destroying our beer supply #survival #sandy #frankenstorm @kimmiefg @krystinlemieux\n",
      "----------\n",
      "@stephgrillz angy poooos house is getting destroyed\n",
      "----------\n",
      "power. i want it. #frankenstorm\n",
      "----------\n",
      "the hurricane sandy twitter parody is terrible\n",
      "----------\n",
      "@billycury: top 3 fav games this week: borderlands 2, the walking dead, skyrim dusted it off #billystopgames #woohwaah\n",
      "----------\n",
      "storms are creepy i need my bella \n",
      "----------\n",
      "sandbags in place at the 205 north location. praying that all of that was unnecessary. #noflood\n",
      "----------\n",
      "enjoying my power outage evening. thank for a roof over my head.\n",
      "----------\n",
      "blasting @hardwell #thecreator \n",
      "----------\n",
      "@sorrynotsoorry we arent allowed to drive unless its an emergency.\n",
      "----------\n",
      "@willwilkinson: no valid government-issued picture id, no rescue. but you cant buy cigarrettes without an id, so ....\n",
      "----------\n",
      "thoughts are with staff amp 200 patients evacuating nyu hospital after generator failure. #hurricanesandy #nyc\n",
      "----------\n",
      "this hurricane wasnt even bad\n",
      "----------\n",
      "the walking dead is 75% off at @steam_games get it before nov 1\n",
      "----------\n",
      "@ladygaga thanks so much for ur prayers still have a long way to go with storm just hanging in there love you\n",
      "----------\n",
      "i hope everyones okay during this storm. my power went out but have the generator goiing.\n",
      "----------\n",
      "to bad hurricane sandy wasnt big enough to wipeout the whole world.\n",
      "----------\n",
      "everyone stay safe #scbd #hurricanesandy #goodnight\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in test_disaster_nonrel['text'].head(50):\n",
    "    print(i)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets That Were Identified as Regular, Non-Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "didnt even text me\n",
      "----------\n",
      "i dont understand bitch\n",
      "----------\n",
      "@marylandprobz i gotta get one of those where can i\n",
      "----------\n",
      "couldnt be more excited \n",
      "----------\n",
      "@hitmanholla @scizas word stone cold.open a can of whip ass on dat boy and dump budweiser on him after it...lol\n",
      "----------\n",
      "ride or die .. we gon make it out this hood\n",
      "----------\n",
      "i dont understand bitch lol\n",
      "----------\n",
      "knock knock bitch im in the house now whats up \n",
      "----------\n",
      "@rm9_era lmao why you wanna do that haha\n",
      "----------\n",
      "i just messaged you cause i was bored yea im sure thats the reason\n",
      "----------\n",
      "@kweadilovewale yo ijus turned my phone on. my power is out so im tryin save battery\n",
      "----------\n",
      "@_percyyy good one -_-\n",
      "----------\n",
      "listening to @brothalynchhung season of da siccness right now.\n",
      "----------\n",
      "@mikepereira would he be considered a defenseless receiver\n",
      "----------\n",
      "just went outside to check out sandy, pretty scary stuff, trees on top of cars etc.\n",
      "----------\n",
      "#100thingsaboutme if i could wear a tiedye shirt and sweats every single day of my life, you had better believe i would.\n",
      "----------\n",
      "@leeeexis @xburke13x  omggg.. where are these pics\n",
      "----------\n",
      "@classictwenty3 he makes me upset\n",
      "----------\n",
      "@rookiewriter8 seems like hk is the safest place to be tonight #sandy\n",
      "----------\n",
      "how tf you accidentally vote for romney  lol \n",
      "----------\n",
      ", she a lil bop \n",
      "----------\n",
      "laid 3 little angels down to bed, i just pray demons dont wake up in there places  for school tomorrow lol help aunt jesse lol\n",
      "----------\n",
      "@kradick22 im not complaining it was a simple tweet\n",
      "----------\n",
      "@darth_talbert its going on right now lol\n",
      "----------\n",
      "@halenspencer yes and the boys :\n",
      "----------\n",
      "@xxjransom2xx @thagavster youre damn right just the thought of you coming here gets me rock solid.\n",
      "----------\n",
      "the haves and the have nots in manhattan. the view from my blacked out hotel room on west 16th. \n",
      "----------\n",
      "sandy turn off de lights\n",
      "----------\n",
      "hearing that nyc has plunged into darkness puts my stomach in a knot\n",
      "----------\n",
      "her leo self better not take advantage of his pisces kindness\n",
      "----------\n",
      "its crazy how bored a person can get from doing absolutely nothing\n",
      "----------\n",
      "@burnymacc @roxinoel94 i love my jack haha\n",
      "----------\n",
      "lets just pretend that this was all just a dream.\n",
      "----------\n",
      "i didnt change. you just never knew me.\n",
      "----------\n",
      "temp: 39.7f  wind:n at 8.7kts barometer: 992.2mb and falling quickly rain since midnight: 1.29in relative humidity: 95%  #wvwx\n",
      "----------\n",
      "@jboochh high tides over water should be receding now  good to know everyones safe\n",
      "----------\n",
      "i cant let these little things slip out of my mouth... cause its you, oh its you, its you they add up tolt3\n",
      "----------\n",
      "guess its time to go to bed now.  hopefully the power will be back soon.\n",
      "----------\n",
      "@mgee_x3  family guyyyyy\n",
      "----------\n",
      "@katie_newport @allisonrockey yay were good. were having a brown out-low flickering lights. its like 1873 up here.\n",
      "----------\n",
      "please think before you speak\n",
      "----------\n",
      "eff too bad this episode was on the other night\n",
      "----------\n",
      "@23solesandhoes she not ugly tho\n",
      "----------\n",
      "fail.\n",
      "----------\n",
      "she gonna say im late\n",
      "----------\n",
      "one ice down.... one to go #drunkyyyy\n",
      "----------\n",
      "@boxing he should rematch quartey, tito, or mosley.\n",
      "----------\n",
      "@johnpattwell yeah same, now we need to drink cause ive got nothing else to do\n",
      "----------\n",
      "choke my chicken, slap my wanker, a girl lets me watch her with her tits out when im spankin then i thank her\n",
      "----------\n",
      "i wish i had friends to stay up with #sigh\n",
      "----------\n",
      "so my apartment is an island right now, the 1st floor is submerged. in for the long haul.\n",
      "----------\n",
      "@alexe747 maybe ill unfollow you just so you have to get 2 more #jksss gt:\n",
      "----------\n",
      "man im missing the bad girls club .\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for i in test_regular['text'].head(50):\n",
    "    print(i)\n",
    "    print('----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Combined Landfall Dataset with Class Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to build out a cleaned, combined dataset of our Sandy landfall tweets to be able to use the geo-tags we have to build a map where we can easily represent how disaster and critical tweets can be visualized amid the sea of all tweets.  For this we are returning to our main data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the portion of Sandy Landfall Tweets (our main pull) that was not labeled \n",
    "#as disaster tweets by either the Figure 8 Model or the Keyword Filtering - so regular tweets.\n",
    "regular_tweets = pd.read_csv('./regular_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263097187775434752</td>\n",
       "      <td>if anybody needs a 2nd shift job and can pass ...</td>\n",
       "      <td>Tue Oct 30 01:57:12 +0000 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 146665477, 'id_str': '146665477', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263097188446523393</td>\n",
       "      <td>of course when the voice is on, my tv decides ...</td>\n",
       "      <td>Tue Oct 30 01:57:12 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-81.33815822...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.15005871,...</td>\n",
       "      <td>{'id': '45a0ea3329c38f9f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 65144874, 'id_str': '65144874', 'name':...</td>\n",
       "      <td>{'hashtags': [{'text': 'ThanksSandy', 'indices...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263097191260901378</td>\n",
       "      <td>@ken_fedor oh hell yeah. we need too</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-74.10466037...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.87194808,...</td>\n",
       "      <td>{'id': '86fc60f26e1639cc', 'url': 'https://api...</td>\n",
       "      <td>{'id': 331299957, 'id_str': '331299957', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263097191881646080</td>\n",
       "      <td>ok...  enough sandy, time to go away.  no real...</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-77.092894, ...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [38.978183, -...</td>\n",
       "      <td>{'id': '864ff125241f172f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 302586627, 'id_str': '302586627', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263097193530023936</td>\n",
       "      <td>niggas not loyal</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-81.6315227,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.5384851, ...</td>\n",
       "      <td>{'id': '0eb9676d24b211f1', 'url': 'https://api...</td>\n",
       "      <td>{'id': 390423015, 'id_str': '390423015', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263097187775434752  if anybody needs a 2nd shift job and can pass ...   \n",
       "1  263097188446523393  of course when the voice is on, my tv decides ...   \n",
       "2  263097191260901378               @ken_fedor oh hell yeah. we need too   \n",
       "3  263097191881646080  ok...  enough sandy, time to go away.  no real...   \n",
       "4  263097193530023936                                  niggas not loyal    \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Oct 30 01:57:12 +0000 2012   \n",
       "1  Tue Oct 30 01:57:12 +0000 2012   \n",
       "2  Tue Oct 30 01:57:13 +0000 2012   \n",
       "3  Tue Oct 30 01:57:13 +0000 2012   \n",
       "4  Tue Oct 30 01:57:13 +0000 2012   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0                                                NaN   \n",
       "1  {'type': 'Point', 'coordinates': [-81.33815822...   \n",
       "2  {'type': 'Point', 'coordinates': [-74.10466037...   \n",
       "3  {'type': 'Point', 'coordinates': [-77.092894, ...   \n",
       "4  {'type': 'Point', 'coordinates': [-81.6315227,...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0                                                NaN   \n",
       "1  {'type': 'Point', 'coordinates': [41.15005871,...   \n",
       "2  {'type': 'Point', 'coordinates': [40.87194808,...   \n",
       "3  {'type': 'Point', 'coordinates': [38.978183, -...   \n",
       "4  {'type': 'Point', 'coordinates': [41.5384851, ...   \n",
       "\n",
       "                                               place  \\\n",
       "0                                                NaN   \n",
       "1  {'id': '45a0ea3329c38f9f', 'url': 'https://api...   \n",
       "2  {'id': '86fc60f26e1639cc', 'url': 'https://api...   \n",
       "3  {'id': '864ff125241f172f', 'url': 'https://api...   \n",
       "4  {'id': '0eb9676d24b211f1', 'url': 'https://api...   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'id': 146665477, 'id_str': '146665477', 'name...   \n",
       "1  {'id': 65144874, 'id_str': '65144874', 'name':...   \n",
       "2  {'id': 331299957, 'id_str': '331299957', 'name...   \n",
       "3  {'id': 302586627, 'id_str': '302586627', 'name...   \n",
       "4  {'id': 390423015, 'id_str': '390423015', 'name...   \n",
       "\n",
       "                                            entities  in_reply_to_user_id  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "1  {'hashtags': [{'text': 'ThanksSandy', 'indices...                  NaN   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "\n",
       "  lang  predicted  tag  \n",
       "0   en          0    0  \n",
       "1   en          0    0  \n",
       "2   en          0    0  \n",
       "3   en          0    0  \n",
       "4   en          0    0  "
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Tweets - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates target column for regular tweets, all of which should be 0\n",
    "regular_tweets['tag'] = np.zeros(len(regular_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105804, 12)"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "text                       4\n",
       "created_at                 2\n",
       "coordinates            10378\n",
       "geo                    10378\n",
       "place                   3170\n",
       "user                      24\n",
       "entities                   2\n",
       "in_reply_to_user_id    69816\n",
       "lang                      27\n",
       "predicted                  0\n",
       "tag                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_tweets.dropna(subset=['text','created_at'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english language only\n",
    "regular_tweets = regular_tweets[regular_tweets['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "regular_tweets['datetime'] = pd.to_datetime(regular_tweets['created_at'])\n",
    "regular_tweets = regular_tweets.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "regular_tweets = regular_tweets[['id','text','datetime','geo','predicted','tag']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "regular_tweets['text'] = regular_tweets['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "regular_tweets.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "regular_tweets['text'] = regular_tweets['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "regular_tweets.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "regular_tweets['id'] = regular_tweets['id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Regular and Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled = pd.read_csv('disaster_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>geo</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>@godseyg i was arrested about 36 hours later. ...</td>\n",
       "      <td>2012-10-30 00:00:02</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12081393,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>wish i was wiff my love during this disaster #...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.28866479,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>im a hurricane vet.. so #hurricanesandy isnt a...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12088316,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>the scorpions - rock you like a hurricane #201...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.60095122,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>lights out @ frankenstorm apocalypse - hurrica...</td>\n",
       "      <td>2012-10-30 00:00:06</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.79093941,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  2.630677e+17  @godseyg i was arrested about 36 hours later. ...   \n",
       "1  2.630677e+17  wish i was wiff my love during this disaster #...   \n",
       "2  2.630677e+17  im a hurricane vet.. so #hurricanesandy isnt a...   \n",
       "3  2.630677e+17  the scorpions - rock you like a hurricane #201...   \n",
       "4  2.630677e+17  lights out @ frankenstorm apocalypse - hurrica...   \n",
       "\n",
       "              datetime                                                geo  \\\n",
       "0  2012-10-30 00:00:02  {'type': 'Point', 'coordinates': [39.12081393,...   \n",
       "1  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [41.28866479,...   \n",
       "2  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [39.12088316,...   \n",
       "3  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [42.60095122,...   \n",
       "4  2012-10-30 00:00:06  {'type': 'Point', 'coordinates': [40.79093941,...   \n",
       "\n",
       "   predicted  tag  \n",
       "0        1.0  0.0  \n",
       "1        1.0  0.0  \n",
       "2        1.0  0.0  \n",
       "3        1.0  0.0  \n",
       "4        1.0  0.0  "
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines disaster-related and regular tweets into a cleaned dataset containing all three classes\n",
    "#(Regular, Disaster Non-Critical, and Disaster Critical)\n",
    "#The convenience of this dataset is it can easily be manipulated to have a class column with all three types labeled.\n",
    "#This will be useful for geo-mapping visualizations.\n",
    "sandy_combined = pd.concat([disaster_labeled,regular_tweets],ignore_index=True)\n",
    "sandy_combined['datetime'] = pd.to_datetime(sandy_combined['datetime'])\n",
    "sandy_combined = sandy_combined.sort_values('datetime')\n",
    "sandy_combined['id'] = sandy_combined['id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_combined.to_csv('./sandy_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geomapping Landfall Data in Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first geomap, we want to map the full body of tweets (regular, disaster, and critical) from the main pull, ie all geotagged tweets from the northeastern seaboard from the time of Sandy landfall in NJ/NY to about 15 hours later.  Some additional processing needs to be done.  Also, not all tweets in our pull have precise coordinates, although most do.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geomap Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap = pd.read_csv('sandy_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all rows that don't have coordinates\n",
    "sandy_geomap = sandy_geomap[sandy_geomap['geo'].notnull()==True]\n",
    "\n",
    "#Splits geo column into latitude and longitude columns\n",
    "sandy_geomap['geo'] = sandy_geomap['geo'].map(lambda x: x.split('[')[1].split(']')[0])\n",
    "sandy_geomap['latitude'] = sandy_geomap['geo'].map(lambda x: x.split(',')[0])\n",
    "sandy_geomap['longitude'] = sandy_geomap['geo'].map(lambda x: x.split(',')[1])\n",
    "sandy_geomap['latitude'] = sandy_geomap['latitude'].map(lambda x: float(x))\n",
    "sandy_geomap['longitude'] = sandy_geomap['longitude'].map(lambda x: float(x))\n",
    "\n",
    "#Condenses to relevant information for geomapping\n",
    "sandy_geomap = sandy_geomap[['id','text','latitude','longitude','datetime','predicted','tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 94043 entries, 0 to 104272\n",
      "Data columns (total 7 columns):\n",
      "id           94043 non-null int64\n",
      "text         94043 non-null object\n",
      "latitude     94043 non-null float64\n",
      "longitude    94043 non-null float64\n",
      "datetime     94043 non-null object\n",
      "predicted    94043 non-null float64\n",
      "tag          94043 non-null float64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "sandy_geomap.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap['class'] = sandy_geomap['predicted'] + sandy_geomap['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    86060\n",
       "1.0     7297\n",
       "2.0      686\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_geomap['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap.to_csv('sandy_geomap.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94043"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sandy_geomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263067699821821952</td>\n",
       "      <td>i should probably start doing my hw</td>\n",
       "      <td>43.104663</td>\n",
       "      <td>-75.127981</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263067700270596096</td>\n",
       "      <td>good thing i practiced my i totally understand...</td>\n",
       "      <td>39.166361</td>\n",
       "      <td>-84.606048</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263067699133947904</td>\n",
       "      <td>@day_hammonds you already know</td>\n",
       "      <td>34.997820</td>\n",
       "      <td>-80.090215</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263067696235692032</td>\n",
       "      <td>@tiffany_niccole im sitting at this gate a6 to...</td>\n",
       "      <td>39.998155</td>\n",
       "      <td>-82.884330</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263067699213660160</td>\n",
       "      <td>girls who go commandogtgt</td>\n",
       "      <td>39.923918</td>\n",
       "      <td>-75.173551</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263067699821821952                i should probably start doing my hw   \n",
       "1  263067700270596096  good thing i practiced my i totally understand...   \n",
       "2  263067699133947904                    @day_hammonds you already know    \n",
       "3  263067696235692032  @tiffany_niccole im sitting at this gate a6 to...   \n",
       "4  263067699213660160                          girls who go commandogtgt   \n",
       "\n",
       "    latitude  longitude             datetime  predicted  tag  class  \n",
       "0  43.104663 -75.127981  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "1  39.166361 -84.606048  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "2  34.997820 -80.090215  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "3  39.998155 -82.884330  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "4  39.923918 -75.173551  2012-10-30 00:00:01        0.0  0.0    0.0  "
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_geomap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Disaster-Related and Critical Tweets - Hurricane Sandy Landfall - Northeast Region (Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class='tableauPlaceholder' id='viz1547848544212' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848544212');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div class='tableauPlaceholder' id='viz1547848544212' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848544212');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Mapping Disaster Critical Tweets in the NYC Area during Hurricane Sandy Landfall (Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class='tableauPlaceholder' id='viz1547848675634' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NYCSandyTweetMap&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848675634');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div class='tableauPlaceholder' id='viz1547848675634' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NYCSandyTweetMap&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848675634');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Streaming Disaster Tweets via Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to return at the end to the root purpose of the project and see what we could do with applying our model to real-time tweets. The primary difficulty here was just obtaining the tweets, but we were able to develop a method using Twython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@ZachariusD 1. Super Mario 64 (rest are in no particular order)\n",
      "- DOOM\n",
      "- Fire Emblem: Genealogy of the Holy War\n",
      "- E‚Ä¶ https://t.co/FwNQoKxviM\n",
      "1\n",
      "RT @smittyselitte: 14. Brian Cashman decided to fire Joe Girardi after Gary Sanchez was benched and their feud reached the media after his‚Ä¶\n",
      "2\n",
      "RT @fienixtaranova: Stop saying ‚ÄúI‚Äôm bored‚Äù and go learn a skill. Web design. Gut a fish. Start a fire without matches or a lighter. Coding‚Ä¶\n",
      "3\n",
      "RT @YouSoThotful: Bobby Shmurda's hat https://t.co/QBCU5LwphX\n",
      "4\n",
      "Delete this then fire the author\n",
      "5\n",
      "RT @Justiiin_Ug: ‚òÖ Some of you don‚Äôt even know the kind of fire coming your way and it shows https://t.co/oJWBlnNiLs\n",
      "6\n",
      "RT @sideheadlock: DLC characters from the last game get nerfed\n",
      "\n",
      "Bayo mains: This community is toxic and fuck y'all\n",
      "\n",
      "*proceeds to retire and‚Ä¶\n",
      "7\n",
      "RT @Jacquees: Preciate it pussy https://t.co/izvH2Uh2MJ\n",
      "8\n",
      "its the hulk\n",
      "9\n",
      "RT @YouSoThotful: Bobby Shmurda's hat https://t.co/QBCU5LwphX\n",
      "10\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: './Live_tweets/Live Tweets 2019-03-22 17:39:40.311378.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d25685a51ee0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mstream\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatuses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'fire'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\twython\\streaming\\types.py\u001b[0m in \u001b[0;36mfilter\u001b[1;34m(self, **params)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://stream.twitter.com/%s/statuses/filter.json'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m               \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstreamer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'POST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\twython\\streaming\\api.py\u001b[0m in \u001b[0;36m_request\u001b[1;34m(self, url, method, params)\u001b[0m\n\u001b[0;32m    152\u001b[0m                                       not valid JSON.')\n\u001b[0;32m    153\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m                         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_success\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m                             \u001b[1;32mfor\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandlers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                                 \u001b[1;32mif\u001b[0m \u001b[0mmessage_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-d25685a51ee0>\u001b[0m in \u001b[0;36mon_success\u001b[1;34m(self, data, ids, texts, geos, created_ats, df, count)\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'geo'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgeos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreated_ats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./Live_tweets/Live Tweets \"\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\".csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'CSV SAVED'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   1743\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1744\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 1745\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1746\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1747\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    155\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    157\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    401\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: './Live_tweets/Live Tweets 2019-03-22 17:39:40.311378.csv'"
     ]
    }
   ],
   "source": [
    "#This set of code uses Twython to stream tweets with disaster keywords in real-time.  \n",
    "#It also compiles these incoming tweets into dataframes of n=100, for potential analysis with our model.\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data, ids=[], texts=[],geos=[],created_ats=[],df = pd.DataFrame(columns=['id','text','geo','created_at']),count=0 ):\n",
    "        print(data['text'])\n",
    "        now = datetime.datetime.now()\n",
    "        ids.append(data['id'])\n",
    "        texts.append(data['text'])\n",
    "        geos.append(data['geo'])\n",
    "        created_ats.append(data['created_at'])\n",
    "        count = len(texts)\n",
    "        print(count)\n",
    "        if count == 10:\n",
    "            df['id'] = ids\n",
    "            df['text'] = texts\n",
    "            df['geo'] = geos\n",
    "            df['created_at'] = created_ats\n",
    "            df.to_csv(\"./Live_tweets/Live Tweets \"+ str(now)+\".csv\")\n",
    "            print('CSV SAVED')\n",
    "            count = 0\n",
    "            df = pd.DataFrame(columns=['id','text','geo','created_at'])\n",
    "        \n",
    "stream = MyStreamer(app_key='Ed2Wq0e44bvHtrsmZGggAs3Lx',\n",
    "                    app_secret='LhSDbftqQXjrQIZK9z5BqIOVoZjmEejsliMyt6C4xeHbjecCst',\n",
    "                    oauth_token='269947090-jBfugJosB1EGnwu5rAFH1a6Ehrc6SAt1T2TjrR9b',\n",
    "                    oauth_token_secret='rQffeEZKG62z7WIRmtFWHKIWZkiJtkU4CfJyvlhrmSwMx')\n",
    "\n",
    "\n",
    "stream.statuses.filter(track= ['fire'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our primary takeaways in this project thus far are as follows:\n",
    "\n",
    "Despite initial setbacks, we were able to arrive at a satisfying and effective two-phase process for identifying critical disaster tweets out of the sea of all incoming tweets on social media. We were pleased with how well we were able to isolate potentially relevant tweets from a sea of test data. We also successfully visualized what an interface for receiving geotagged tweets in real-time might look like, although we expect a more integrated and multi-functional mapping software than Tableau might be necessary for real-world implementation. Finally, we were able to demonstrate proof-of-concept on live-streaming capture of tweets. \n",
    "\n",
    "Given unrestricted access to the data available via Twitter, let alone Facebook, Snapchat, Instagram (all of which FEMA or a similar organization would likely have in the hypothetical scenario where they would implement this process), I feel we have demonstrated that it would be absolutely possible to build out a very useful and accurate geo-feed of emergency response information in the area of an ongoing disaster.\n",
    "\n",
    "An ideal implementation perhaps would be to continually label critical tweets after the fact and progressively train for different types of emergency tweets over many documents and events.  Emergency personnel whose job it is to review the incoming tweets that the existing model has identified as critical could also be tasked with manually labeling the identified tweets as truly critical or not as they come in, with this labeling feeding back into actively improving the performance of the model as time goes on.  With enough development, we could reach a point where we have specialized models for different disaster types, which emergency personnel could turn on as appropriate once a disaster scenario is live.\n",
    "\n",
    "All of this bodes well for future expansion of the project. Other directions we might go with more time include attempting to involve another social media platform, or locate a database of tweets from a disaster scenario other than a hurricane to try and diversify our filtering process. We also would like to try and improve predictive accuracy by use of Words2Vec, which is well equipped to locate similar types of tweets (e.g. critical tweets) without explicit labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
