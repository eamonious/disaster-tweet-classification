{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----------------------------------------\n",
    "\n",
    "\n",
    "# Leveraging Social Media To Alert Emergency Response Personnel During Disasters\n",
    "\n",
    "### Contents:\n",
    "-------------------------------\n",
    "- [Problem Statement](#Problem-Statement)\n",
    "- [Importing Libraries](#Importing-Libraries)\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Pulling Tweets with Twython](#Pulling-Tweets-with-Twython)\n",
    "- [Filtering for Disaster Tweets: Figure Eight Model](#Filtering-for-Disaster-Tweets:-Figure-Eight-Model)\n",
    "- [Filtering for Disaster Tweets: Crisis Words](#Filtering-for-Disaster-Tweets:-Crisis-Words)\n",
    "- [Apply Filters to Generate List of Disaster Tweets](#Apply-Filters-to-Generate-List-of-Disaster-Tweets)\n",
    "- [Identifying Critical Tweets Among Disaster Tweets](#Identifying-Critical-Tweets-Among-Disaster-Tweets)\n",
    "- [Testing 2-Phase Model (1. Disaster Filter, 2. Predict Critical) on New Data](#Testing-2-Phase-Model-{1.-Disaster-Filter,-2.-Predict-Critical}-on-New-Data)\n",
    "- [Create Combined Landfall Dataset with Class Labeling](#Create-Combined-Landfall-Dataset-with-Class-Labeling)\n",
    "- [Geomapping Landfall Data in Tableau](#Geomapping-Landfall-Data-in-Tableau)\n",
    "- [Live Streaming Disaster Tweets via Twython](#Live-Streaming-Disaster-Tweets-via-Twython)\n",
    "- [Conclusions:](#Conclusions)\n",
    "-------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this project we sought to make initial steps toward designing and implementing a web-tool or an app for rapid alert and notification about a disastrous event, in close to real time. The tool ideally would alert about the event, similar, for example, to earthquake or tsunami warnings that appear on Google's home page immediately after a major disaster. While traditional methods for alerting on such events rely on official information derived from official sources (e.g. USGS), this tool will utilize social media activity to identify these events and alert when an event first occurs.\n",
    "\n",
    "The question we look at primarily here is, given a sea of text content from social media platforms, how do you identify what is relevant information for emergency response personnel?  And what sort of implementation would be valuable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from twython import Twython\n",
    "from tqdm import *\n",
    "from time import sleep\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import re\n",
    "from wordcloud import WordCloud\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our initial plan was to try and real-time map incoming tweets using Twitter, Facebook, Instagram, Snapchat, etc.  We quickly found that the APIs for these social media platforms have become much more restrictive than they used to be.  The limits on how many tweets we could get at one time were such that it wasn't feasible to build out a dataset that would support training a model.  We also could not get geolocation.  We needed a different approach.\n",
    "\n",
    "We found a useful dataset on the website CrisisLex, which collects datasets specific to NLP applications in disaster scenarios. This dataset contained tweet IDs for all geotagged tweets (6 million +) from affected areas of the Eastern Seaboard during the 11 day period surrounding Hurricane Sandy's landfall (10/22/2012-11/2/2012).  The tweets include all content, not just disaster-related tweets.  \n",
    "\n",
    "You can use specific tweet IDs to pull dictionaries corresponding to the specific tweets.  Having specific tweet IDs also allows you to collect up to 900 tweets every fifteen minutes, which is much larger than the normal limit.  The fact that geotags with precise coordinates of sending location and timestamps to the second are included in the data is also very relevant, as it allows live-mapping, and accurately simulates the kind of information we would expect FEMA to have access to in this type of real-world application scenario.\n",
    "\n",
    "Hurricanes, of all disasters, are probably the best-equipped to represent a generalizable lexicon, as hurricanes often involve a combination of flooding, fires, building damage/collapse, sufficient wind to down trees, explosions, injuries, deaths, trapped/stranded individuals, etc.  Obviously, the training data could be expanded to include a variety of disaster types in the future.\n",
    "\n",
    "Further Information on the CrisisLex Sandy Tweet ID Dataset:\n",
    "\n",
    "- [CrisisLex: SandyHurricaneGeoT1 Geo-Located tweets from the 2012 Sandy Hurricane](https://crisislex.org/data-collections.html#SandyHurricaneGeoT1)\n",
    "- Contents: tweet ids for 6,556,328 tweets, representing all tweets from October 22nd, 2012 ‚Äîthe day Sandy formed‚Äî until November 2nd, 2012 ‚Äî the day that it dissipated.\n",
    "- Sampling method: tweets were geotagged and located in Washington DC or one of 13 US states affected by Sandy: Connecticut, Delaware, Massachusetts, Maryland, New Jersey, New York, North Carolina, Ohio, Pennsylvania, Rhode Island, South Carolina, Virginia,West Virginia. This filter was based on a set of bounding boxes that covered the desired area, which also covered small parts of adjacent states.\n",
    "- Labels: no labels. The corpus contains tweets both relevant and irrelevant to Hurricane Sandy (no content based filter was applied).\n",
    "- Data format: comma-separated values (.csv) files containing the tweet ID, the time stamp of the tweet, a field indicating whether the tweet contains word \"sandy\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Sandy ID List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../release.txt',sep= ' ', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tag:search.twitter.com,2005:260244087901413376...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088203403264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088161439744...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tag:search.twitter.com,2005:260244088819945472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tag:search.twitter.com,2005:260244089080004609...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  tag:search.twitter.com,2005:260244087901413376...\n",
       "1  tag:search.twitter.com,2005:260244088203403264...\n",
       "2  tag:search.twitter.com,2005:260244088161439744...\n",
       "3  tag:search.twitter.com,2005:260244088819945472...\n",
       "4  tag:search.twitter.com,2005:260244089080004609..."
      ]
     },
     "execution_count": 667,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6554744, 1)"
      ]
     },
     "execution_count": 564,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into respective columns, create datetime column, drop unnecessary columns\n",
    "\n",
    "df = data[0].map(lambda x: x.split('\\t'))\n",
    "df = pd.DataFrame(df)\n",
    "df['timestamp'] = df[0].map(lambda x: x[1])\n",
    "df['tweet_id'] = df[0].map(lambda x: x[0])\n",
    "df['bool'] = df[0].map(lambda x: x[2])\n",
    "df = df.drop(columns=0)\n",
    "df['tweet_id'] = df['tweet_id'].map(lambda x: x.split(':')[2])\n",
    "df['datetime'] = pd.to_datetime(df['timestamp'])\n",
    "df = df.drop(columns=['timestamp','bool'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>260244088203403264</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>2012-10-22 05:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id            datetime\n",
       "0  260244087901413376 2012-10-22 05:00:00\n",
       "1  260244088203403264 2012-10-22 05:00:00\n",
       "2  260244088161439744 2012-10-22 05:00:00\n",
       "3  260244088819945472 2012-10-22 05:00:00\n",
       "4  260244089080004609 2012-10-22 05:00:00"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sandy_ids_full.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ID List for Sandy Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to simulate the sort of access to Twitter that FEMA would have during a crisis situation, i.e.; all geotagged and timestamped tweets within some period of time.  Moreover, we anticipated that our classes in our final model would be unbalanced, because actually critical disaster tweets would be quite rare.  We wanted to sample from the period that would have as many of these as possible, in order to have more of them to train on and rely less on bootstrapping.\n",
    "\n",
    "Accordingly, we chose to sample from the window surrounding the landfall of Hurricane Sandy in New Jersey and New York (~8PM ET, October 29th, 2012.  We calculated that we could reasonably aim to pull about 180000 tweets for the training set, timewise.  We chose to pull all tweets from the list for the 3 hour period spanning from 1 hour prior to landfall to 2 hours afterward, so approximately 7PM-10PM that night. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftime = df.sort_values('datetime')\n",
    "dftime = dftime.reset_index(drop=True)\n",
    "\n",
    "#Picked the time index corresponding approximately to landfall of the hurricane \n",
    "dftime[dftime['datetime']=='2012-10-30 00:00:01'].head()\n",
    "\n",
    "#creates our major id list, from approximate time of landfall in NJ to about 3 hours later \n",
    "#(i.e., 180000 tweets down the timestamp-sorted ID list), all geotagged tweets in that time\n",
    "#continuous timespan also allows us to show complete minute to minute mapping visualization\n",
    "sandy_id_time = dftime.loc[4428365:4608434,:]\n",
    "sandy_id_time.to_csv('sandy_id_time.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Tweets with Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of us created Twitter Development accounts and submitted applications for the project.  We each created two sets of [Twitter API](https://developer.twitter.com/en/apply-for-access) app keys so that we could pull tweets in tandem to allow for higher volume data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONSUMER_KEY = 'INSERT KEY HERE'\n",
    "CONSUMER_SECRET = 'INSERT KEY HERE'\n",
    "\n",
    "OAUTH_TOKEN = 'INSERT KEY HERE'\n",
    "OAUTH_SECRET = 'INSERT KEY HERE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter = Twython(CONSUMER_KEY, CONSUMER_SECRET, OAUTH_TOKEN, OAUTH_SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling a Single Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260244088161439744"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_id_time['tweet_id'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'created_at': 'Mon Oct 22 05:00:00 +0000 2012',\n",
       " 'id': 260244088161439744,\n",
       " 'id_str': '260244088161439744',\n",
       " 'text': '@NOT_savinHOES Not r yu upp',\n",
       " 'truncated': False,\n",
       " 'entities': {'hashtags': [],\n",
       "  'symbols': [],\n",
       "  'user_mentions': [{'screen_name': 'NOT_savinHOES',\n",
       "    'name': '01.18ü§∏üèΩ\\u200d‚ôÄÔ∏è',\n",
       "    'id': 293455555,\n",
       "    'id_str': '293455555',\n",
       "    'indices': [0, 14]}],\n",
       "  'urls': []},\n",
       " 'source': '<a href=\"http://twitter.com/download/android\" rel=\"nofollow\">Twitter for Android</a>',\n",
       " 'in_reply_to_status_id': None,\n",
       " 'in_reply_to_status_id_str': None,\n",
       " 'in_reply_to_user_id': 293455555,\n",
       " 'in_reply_to_user_id_str': '293455555',\n",
       " 'in_reply_to_screen_name': 'NOT_savinHOES',\n",
       " 'user': {'id': 401231570,\n",
       "  'id_str': '401231570',\n",
       "  'name': 'Jay ü§∑üèΩ\\u200d‚ôÇÔ∏è',\n",
       "  'screen_name': 'JayyLive202',\n",
       "  'location': 'Washington, DC, USA',\n",
       "  'description': '–∫Œπ–∏g  ùŒ±–º—î—ïüé©üèÜ 25. D[M]V | üëª:OfficialJaymes üì∏Insta:Oh.ThatsAlexx',\n",
       "  'url': 'https://t.co/a6Q76c6YNl',\n",
       "  'entities': {'url': {'urls': [{'url': 'https://t.co/a6Q76c6YNl',\n",
       "      'expanded_url': 'https://link.dosh.cash/JAMESR117',\n",
       "      'display_url': 'link.dosh.cash/JAMESR117',\n",
       "      'indices': [0, 23]}]},\n",
       "   'description': {'urls': []}},\n",
       "  'protected': False,\n",
       "  'followers_count': 803,\n",
       "  'friends_count': 576,\n",
       "  'listed_count': 0,\n",
       "  'created_at': 'Sun Oct 30 07:41:09 +0000 2011',\n",
       "  'favourites_count': 326,\n",
       "  'utc_offset': None,\n",
       "  'time_zone': None,\n",
       "  'geo_enabled': True,\n",
       "  'verified': False,\n",
       "  'statuses_count': 18359,\n",
       "  'lang': 'en',\n",
       "  'contributors_enabled': False,\n",
       "  'is_translator': False,\n",
       "  'is_translation_enabled': False,\n",
       "  'profile_background_color': 'C0DEED',\n",
       "  'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
       "  'profile_background_tile': True,\n",
       "  'profile_image_url': 'http://pbs.twimg.com/profile_images/1070198512380461056/zthonwAC_normal.jpg',\n",
       "  'profile_image_url_https': 'https://pbs.twimg.com/profile_images/1070198512380461056/zthonwAC_normal.jpg',\n",
       "  'profile_banner_url': 'https://pbs.twimg.com/profile_banners/401231570/1543990185',\n",
       "  'profile_link_color': '0084B4',\n",
       "  'profile_sidebar_border_color': 'FFFFFF',\n",
       "  'profile_sidebar_fill_color': 'DDEEF6',\n",
       "  'profile_text_color': '333333',\n",
       "  'profile_use_background_image': True,\n",
       "  'has_extended_profile': True,\n",
       "  'default_profile': False,\n",
       "  'default_profile_image': False,\n",
       "  'following': False,\n",
       "  'follow_request_sent': False,\n",
       "  'notifications': False,\n",
       "  'translator_type': 'none'},\n",
       " 'geo': {'type': 'Point', 'coordinates': [40.2371544, -76.8206691]},\n",
       " 'coordinates': {'type': 'Point', 'coordinates': [-76.8206691, 40.2371544]},\n",
       " 'place': {'id': 'b8ce2948ffafff5f',\n",
       "  'url': 'https://api.twitter.com/1.1/geo/id/b8ce2948ffafff5f.json',\n",
       "  'place_type': 'city',\n",
       "  'name': 'Bressler-Enhaut-Oberlin',\n",
       "  'full_name': 'Bressler-Enhaut-Oberlin, PA',\n",
       "  'country_code': 'US',\n",
       "  'country': 'United States',\n",
       "  'contained_within': [],\n",
       "  'bounding_box': {'type': 'Polygon',\n",
       "   'coordinates': [[[-76.831479, 40.22417],\n",
       "     [-76.811937, 40.22417],\n",
       "     [-76.811937, 40.242082],\n",
       "     [-76.831479, 40.242082]]]},\n",
       "  'attributes': {}},\n",
       " 'contributors': None,\n",
       " 'is_quote_status': False,\n",
       " 'retweet_count': 0,\n",
       " 'favorite_count': 0,\n",
       " 'favorited': False,\n",
       " 'retweeted': False,\n",
       " 'lang': 'en'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.show_status(id='260244088161439744')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Pull Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were aware the cap was 900 tweets in 15 minutes.  While we could set up a Twython call to run through some number of indices in our id list, once it hit the 900 limit, it would continue to mow through indices without actually getting anything.  This 900 count includes the significant percentage (~25%) of these old Sandy tweets that have since been deleted and yield no information, they are still counted as tweet calls.  So basically we would move 900 indices through the tweet id list per pull, regardless. \n",
    "\n",
    "We needed a way to automate looping through this pull process at least a few times so as not to end up with some ridiculous number of csvs, and so we could leave things running.  In order to achieve this, we needed the loop to know where to pick up on each new pull."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Goes through a block of 900 indices from the tweet list, from some start index.\n",
    "#Pulls tweet dictionary if tweet exists and adds to lst, otherwise continues to next index.\n",
    "#Returns a list with the start index for the next pull, and the lst containing all the tweet dicts.\n",
    "#Tqdm allows us to track the progress visually with each pull, as seen below.\n",
    "\n",
    "def tweet_pull(start_index):\n",
    "    tweet = None\n",
    "    lst = []\n",
    "    for i in tqdm(range(start_index,start_index+900)):\n",
    "        try:\n",
    "            dct = twitter.show_status(id=str(sandy_id_time['tweet_id'][i]))\n",
    "            lst.append(dct)\n",
    "        except:\n",
    "            tweet = None\n",
    "        sandy_id_time.set_value(i, 'tweet_texts', tweet)\n",
    "    return [start_index+900,lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/900 [00:00<?, ?it/s]C:\\Users\\eamon\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: set_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:19<00:00, 13.37it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:18<00:00, 11.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:25<00:00, 10.57it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 900/900 [01:29<00:00, 10.06it/s]\n"
     ]
    }
   ],
   "source": [
    "#While loop for tweet pulling\n",
    "#Simply set count = [index you want to start at], and set while count < [index you want to end at] (multiple of 900, ideally)\n",
    "#The while loop will run through all the indices in pulls of 900, shifting the start index up 900 each time,\n",
    "#and sleeping 15 minutes after each pull to make sure we are never drawing on empty.\n",
    "#The pulls are added to a single list of dictionaries that can be converted into a df.\n",
    "\n",
    "count = 0\n",
    "tweets = []\n",
    "while count < 3600:    \n",
    "    pull = tweet_pull(count)\n",
    "    tweets.extend(pull[1])\n",
    "    count = pull[0]\n",
    "    sleep(900) #15 minute limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contributors</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>created_at</th>\n",
       "      <th>entities</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>geo</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>lang</th>\n",
       "      <th>place</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>possibly_sensitive_appealable</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [{'text': 'ilovemaggiesmith', 'in...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>260244087901413376</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': 'c55500e8cd2a1c64', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>\"I suppose she has an appropriate costume for ...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 24753438, 'id_str': '24753438', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-76.8206691,...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.2371544, ...</td>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>260244088161439744</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': 'b8ce2948ffafff5f', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@NOT_savinHOES Not r yu upp</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 401231570, 'id_str': '401231570', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-79.20266541...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [34.69318931,...</td>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>260244088819945472</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '6057f1e35bcc6c20', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>Hit and Run is so sad..</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 123368790, 'id_str': '123368790', 'name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-71.04264063...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.44167162,...</td>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>260244089080004609</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '75f5a403163f6f95', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>Who's up?</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 47812293, 'id_str': '47812293', 'name':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>None</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.08961896...</td>\n",
       "      <td>Mon Oct 22 05:00:00 +0000 2012</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.09464892,...</td>\n",
       "      <td>260244089985957888</td>\n",
       "      <td>260244089985957888</td>\n",
       "      <td>...</td>\n",
       "      <td>en</td>\n",
       "      <td>{'id': '29aaa88d9fe74b50', 'url': 'https://api...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>@augustushazel idk I'm just ugly or annoying o...</td>\n",
       "      <td>False</td>\n",
       "      <td>{'id': 274750107, 'id_str': '274750107', 'name...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  contributors                                        coordinates  \\\n",
       "0         None                                               None   \n",
       "1         None  {'type': 'Point', 'coordinates': [-76.8206691,...   \n",
       "2         None  {'type': 'Point', 'coordinates': [-79.20266541...   \n",
       "3         None  {'type': 'Point', 'coordinates': [-71.04264063...   \n",
       "4         None  {'type': 'Point', 'coordinates': [-80.08961896...   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Mon Oct 22 05:00:00 +0000 2012   \n",
       "1  Mon Oct 22 05:00:00 +0000 2012   \n",
       "2  Mon Oct 22 05:00:00 +0000 2012   \n",
       "3  Mon Oct 22 05:00:00 +0000 2012   \n",
       "4  Mon Oct 22 05:00:00 +0000 2012   \n",
       "\n",
       "                                            entities extended_entities  \\\n",
       "0  {'hashtags': [{'text': 'ilovemaggiesmith', 'in...               NaN   \n",
       "1  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...               NaN   \n",
       "\n",
       "   favorite_count  favorited  \\\n",
       "0               0      False   \n",
       "1               0      False   \n",
       "2               0      False   \n",
       "3               0      False   \n",
       "4               0      False   \n",
       "\n",
       "                                                 geo                  id  \\\n",
       "0                                               None  260244087901413376   \n",
       "1  {'type': 'Point', 'coordinates': [40.2371544, ...  260244088161439744   \n",
       "2  {'type': 'Point', 'coordinates': [34.69318931,...  260244088819945472   \n",
       "3  {'type': 'Point', 'coordinates': [42.44167162,...  260244089080004609   \n",
       "4  {'type': 'Point', 'coordinates': [42.09464892,...  260244089985957888   \n",
       "\n",
       "               id_str                        ...                         lang  \\\n",
       "0  260244087901413376                        ...                           en   \n",
       "1  260244088161439744                        ...                           en   \n",
       "2  260244088819945472                        ...                           en   \n",
       "3  260244089080004609                        ...                           en   \n",
       "4  260244089985957888                        ...                           en   \n",
       "\n",
       "                                               place possibly_sensitive  \\\n",
       "0  {'id': 'c55500e8cd2a1c64', 'url': 'https://api...                NaN   \n",
       "1  {'id': 'b8ce2948ffafff5f', 'url': 'https://api...                NaN   \n",
       "2  {'id': '6057f1e35bcc6c20', 'url': 'https://api...                NaN   \n",
       "3  {'id': '75f5a403163f6f95', 'url': 'https://api...                NaN   \n",
       "4  {'id': '29aaa88d9fe74b50', 'url': 'https://api...                NaN   \n",
       "\n",
       "   possibly_sensitive_appealable retweet_count  retweeted  \\\n",
       "0                            NaN             0      False   \n",
       "1                            NaN             0      False   \n",
       "2                            NaN             0      False   \n",
       "3                            NaN             0      False   \n",
       "4                            NaN             0      False   \n",
       "\n",
       "                                              source  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...   \n",
       "1  <a href=\"http://twitter.com/download/android\" ...   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...   \n",
       "4  <a href=\"http://twitter.com/download/android\" ...   \n",
       "\n",
       "                                                text truncated  \\\n",
       "0  \"I suppose she has an appropriate costume for ...     False   \n",
       "1                        @NOT_savinHOES Not r yu upp     False   \n",
       "2                            Hit and Run is so sad..     False   \n",
       "3                                          Who's up?     False   \n",
       "4  @augustushazel idk I'm just ugly or annoying o...     False   \n",
       "\n",
       "                                                user  \n",
       "0  {'id': 24753438, 'id_str': '24753438', 'name':...  \n",
       "1  {'id': 401231570, 'id_str': '401231570', 'name...  \n",
       "2  {'id': 123368790, 'id_str': '123368790', 'name...  \n",
       "3  {'id': 47812293, 'id_str': '47812293', 'name':...  \n",
       "4  {'id': 274750107, 'id_str': '274750107', 'name...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy = pd.DataFrame(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2569, 26)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy.to_csv('####.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We coordinated to split up the job of pulling these loops.  We looped through a total of 180000 tweets from the landfall of Sandy.  We used Google Colab and Google Cloud Computing to run our pull loops over long stretches and collected data into a handful of csvs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining All the Pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.read_csv('alexpulls.csv')\n",
    "data2 = pd.read_csv('eamonpulls.csv')\n",
    "data3 = pd.read_csv('javipulls.csv')\n",
    "\n",
    "data = pd.concat([data1,data2,data3], ignore_index=True)\n",
    "data.drop(columns=['Unnamed: 0'])\n",
    "data.dropna(subset=['id','text','created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_csv('sandy_landfall.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for Disaster Tweets: Figure Eight Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first issues we ran into in approaching our newly collected dataset is scale.  We knew we would have to manually label the critically relevant tweets, however it was unfeasible to search through over 100k tweets to do this.  So how could we whittle the larger body of tweets down to a selection of at least disaster-related tweets that we could then go through manually.\n",
    "\n",
    "We discovered a dataset on the website Figure Eight that had also been posted to Kaggle.  It was essentially a dataset of tweets from the time and location of disaster scenarios which had been labeled for disaster-relevant.  So we figured that by training on this dataset first, we could develop a way to whittle our list down to disaster-related tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme information\n",
    "Datasets:\n",
    "- [Figure Eight - Disasters on social media dataset](https://d1p17r2m4rzlbo.cloudfront.net/wp-content/uploads/2016/03/socialmedia-disaster-tweets-DFE.csv )\n",
    "Contributors looked at over 10,000 tweets culled with a variety of searches like ‚Äúablaze‚Äù, ‚Äúquarantine‚Äù, and ‚Äúpandemonium‚Äù, then noted whether the tweet referred to a disaster event (as opposed to a joke with the word or a movie review or something non-disastrous)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning Kaggle Data (remove http addresses with regex, lowercase the text)\n",
    "df = pd.read_csv('kaggle_dataframe_2.csv')\n",
    "df['text'] = df['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "df['text'] = df['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "df = df[df.duplicated('text')==False]\n",
    "df['text']= df['text'].map(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['text']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y , stratify=y, random_state=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran Logistic Regression GridSearches with multiple vectorizers (Tdidf, Hashing, Count), and found that CountVectorizer performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2, 4, 6],                            #In the paragraph after I will explain the parameters chosen\n",
    "    'vect__ngram_range':[(1,2),(1,3)],\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'model__penalty':['l1','l2'],  \n",
    "    'model__C':[0.01, 0.1 ,1]   \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', RandomForestClassifier() )\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'vect__ngram_range':[(1,2),(1,3)],\n",
    "    'model__n_estimators':[75, 200, 500],\n",
    "    'model__max_depth':[5, 25, 75],\n",
    "    'model__min_samples_split':[2,3,4]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', svm.SVC())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None ,'english'],\n",
    "    'model__kernel':['rbf','poly'],\n",
    "    'model__C':[.1, 1, 10]  \n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[1,2,4, 6],\n",
    "    'vect__stop_words':[None, 'english'],\n",
    "    'model__alpha': [0.1,1,10]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost GridSearch, Multiple Vectorizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We ran a number of different boosting classifiers with multiple vectorizers, of which XG Boost performed the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('model', XGBClassifier())\n",
    "])\n",
    "\n",
    "params = {\n",
    "    'vect__min_df':[2,4,6],\n",
    "    'vect__stop_words':[None,'english'],\n",
    "    'model__n_estimators': [700, 1500],\n",
    "    'model__min_samples_split':[2,4,6],\n",
    "    'model__max_depth':[3,5]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe, params, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best Params: ',gs.best_params_)\n",
    "print('Best Estimator Score Train: ', gs.best_estimator_.score(X_train, y_train))\n",
    "print('Best Estimator Score Test: ', gs.best_estimator_.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VotingClassifier with best LogReg, XGBoost, RandomForest (Final Model Choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ultimately we went with a VotingClassifier that combined the predictive input of multiple models (LogReg, XGBoost, and Random Forest). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Pipeline([\n",
    "        ('count_vect', CountVectorizer(min_df=2,  \n",
    "                                  ngram_range=(1, 3))),     \n",
    "        ('clf', VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                             (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                             #(\"pip3\", svm.SVC(kernel='rbf',C=10,probability=True)),\n",
    "                                             #(\"pip4\", MultinomialNB(alpha=1)), \n",
    "\n",
    "                                             (\"pip5\", RandomForestClassifier(max_depth=75, \n",
    "                                                                             min_samples_split=4, \n",
    "                                                                             n_estimators=200))],voting='soft'))\n",
    "         ])\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our cross-val score for this model was 0.802, against a baseline accuracy of about 0.58.  This is the best result we saw in this modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the VotingClassifier on the Figure Eight Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2, ngram_range = (1,3))\n",
    "\n",
    "X_train_features = vectorizer.fit_transform(X_train)\n",
    "X_test_features = vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "model = VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft')\n",
    "model.fit(X_train_features, y_train)\n",
    "\n",
    "#\n",
    "print('Best Estimator Score Train: ', model.score(X_train_features, y_train))\n",
    "print('Best Estimator Score Test: ', model.score(X_test_features, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test_features)\n",
    "\n",
    "def make_nice_conmat(y_test, preds):\n",
    "\n",
    "    cmat = confusion_matrix(y_test, preds)\n",
    "    print(f'Accuracy: {accuracy_score(y_test, preds)}')\n",
    "    print(classification_report(y_test, preds))\n",
    "    return pd.DataFrame(cmat, columns=['Predicted ' + str(i) for i in ['Regular Tweets','Disaster Tweets']],\\\n",
    "            index=['Actual ' + str(i) for i in ['Regular Tweets','Disaster Tweets']])\n",
    "\n",
    "make_nice_conmat(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"confusionmatrix-figure8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train VotingClassifier on Entire Figure 8 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df=2, ngram_range = (1,3))\n",
    "\n",
    "X_features = vectorizer2.fit_transform(X)\n",
    "\n",
    "\n",
    "model2 = VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft')\n",
    "model2.fit(X_features, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X_features,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions2 = model2.predict(X_features)\n",
    "df['predictions'] = predictions2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud: Words that Help Identify Disaster Tweets from Regular Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This was based on coefficients from the Logistic Regression model:\n",
    "wordclouddf = feats.T\n",
    "wordclouddf = wordclouddf[wordclouddf[0] >= 0][0].map(lambda x:  x**2)\n",
    "wordclouddict = wordclouddf.to_dict()\n",
    "\n",
    "wordcloud = WordCloud(width=800, height=800,background_color='white').generate_from_frequencies(wordclouddict)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/mmfEg7T.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering for Disaster Tweets: Crisis Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were less than completely satisfied with the output of the Kaggle model, and in particular, looking closely at the dataset, felt that some of the labeling was suspect or flat out incorrect.  We decided that a keyword list could be useful in further filtering for tweets of disaster relevance.\n",
    "\n",
    "We located a long list of single words and 2 word pairings (ie AND function, not n-grams) identified as disaster-related keywords on CrisisNLP.org.  \n",
    "\n",
    "We imported this list and made some additions that we felt were relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initial CrisisNLP list imported\n",
    "keywords = 'flood crisis, victims, flood victims, flood powerful, powerful storms, hoisted flood, storms amazing, explosion, amazing rescue, rescue women, flood cost, counts flood, toll rises, braces river, river peaks, crisis deepens, prayers, thoughts prayers, affected tornado, affected, death toll, tornado relief, photos flood, water rises, toll, flood waters, flood appeal, victims explosion, bombing suspect, massive explosion, affected areas, praying victims, injured, please join, join praying, prayers people, redcross, text redcross, visiting flood, lurches fire, video explosion, deepens death, opposed flood, help flood, died explosions, marathon explosions, flood relief, donate, first responders, flood affected, donate cross, braces, tornado victims, deadly, prayers affected, explosions running, evacuated, relief, flood death, deaths confirmed, affected flooding, people killed, dozens, footage, survivor finds, worsens eastern, flood worsens, flood damage, people dead, girl died, flood, donation help, major flood, rubble, another explosion, confirmed dead, rescue, send prayers, flood warnings, tornado survivor, damage, devastating, flood toll, affected hurricane, prayers families, releases photos, hundreds injured, inundated, crisis, text donation, redcross give, recede, bombing, massive, bombing victims, explosion ripped, gets donated, donated victims, relief efforts, news flood, flood emergency, give online, fire flood, huge explosion, bushfire, torrential rains, residents, breaking news, redcross donate, affected explosion, disaster, someone captured, tragedy, enforcement, people injured, twister, blast, crisis deepens, injuries reported, fatalities, donated million, donations assist, dead explosion, survivor, death, suspect dead, peaks deaths, love prayers, explosion fertiliser, explosion reported, return home, evacuees, large explosion, firefighters, morning flood, praying, public safety, txting redcross, destroyed, displaced, fertilizer explosion, unknown number, donate tornado, retweet donate, flood tornado, casualties, climate change, financial donations, stay strong, dead hundreds, major explosion, bodies recovered, waters recede, response disasters, victims donate, unaccounted, fire fighters, explosion victims, prayers city, accepting financial, torrential, bomber, disasters txting, explosion registered, missing flood, volunteers, brought hurricane, relief fund, help tornado, explosion fire, ravaged, prayers tonight, tragic, enforcement official, saddened, dealing hurricane, impacted, flood recovery, stream, dead torrential, flood years, nursing, recover, responders, massive tornado, buried alive, alive rubble, crisis rises, flood peak, homes inundated, flood ravaged, explosion video, killed injured, killed people, people died, missing explosion, make donation, floods kill, tornado damage, entire crowd, cross tornado, terrifying, need terrifying, even scary, cost deaths, facing flood, deadly explosion, dead missing, floods force, flood disaster, tornado disaster, medical examiner, help victims, hundreds homes, severe flooding, shocking video, bombing witnesses, magnitude, firefighters police, fire explosion, storm, flood hits, floodwaters, emergency, flash flood, flood alerts, crisis unfolds, daring rescue, tragic events, medical office, deadly tornado, people trapped, police officer, explosion voted, lives hurricane, bombings reports, breaking suspect, bombing investigation, praying affected, reels surging, surging floods, teenager floods, rescue teenager, appeal launched, explosion injured, injured explosion, responders killed, explosion caught, city tornado, help text, name hurricane, damaged hurricane, breaking arrest, suspect bombing, massive manhunt, releases images, shot killed, rains severely, house flood, live coverage, devastating tornado, lost lives, reportedly dead, following explosion, remember lives, tornado flood, want help, seconds bombing, reported dead, imminent, rebuild, safe hurricane, surviving, injuries, prayers victims, police suspect, warning, help affected, kills forces, dead floods, flood threat, military, flood situation, thousands homes, risk running, dead injured, dying hurricane, loss life, thoughts victims, bombing shot, breaking enforcement, police people, video capturing, feared dead, terrible explosion, prayers involved, reported injured, seismic, victims waters, flood homeowners, flood claims, homeowners reconnect, reconnect power, power supplies, rescuers help, free hotline, hotline help, please stay, investigation, saddened loss, identified suspect, bombings saddened, killed police, dead, praying community, registered magnitude, leave town, reported explosion, heart praying, life heart, prepare hurricane, landfall, crisis worsens, arrest, bombing case, suspect run, communities damaged, destruction, levy, tornado, hurricane coming, toxins flood, release toxins, toxins, supplies waters, crisis found, braces major, government negligent, attack, hurricane, rebuilt communities, help rebuilt, rebuilt, rescuers, buried, heart prayers, flood levy, watch hurricane, victims lost, soldier, waiting hurricane, run massive, high river, terror, memorial service, terror attack, coast hurricane, terrified hurricane, aftermath, suspect killed, suspect pinned, lost legs, hurricane category, names terrified, authorities, assist people, hurricane black, unknown soldier, events, safety, troops, disaster relief, cleanup, troops lend, effected hurricane, time hurricane, saying hurricane, praying families, dramatic, path hurricane, crash, drown, outage, looting, flooding, help trouble, help need, help!, power outage, power line, shelter, collapse, collapsed, wounded, dangerous, conditions, urgent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "#arrange alphabetically and split\n",
    "keys = pd.DataFrame(keywords.split(', '))\n",
    "keys = keys.sort_values(0)\n",
    "keys = keys.reset_index(drop=True)\n",
    "keys['keywords'] = keys[0].map(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>accepting financial</td>\n",
       "      <td>[accepting, financial]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>affected</td>\n",
       "      <td>[affected]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>affected areas</td>\n",
       "      <td>[affected, areas]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>affected explosion</td>\n",
       "      <td>[affected, explosion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>affected flooding</td>\n",
       "      <td>[affected, flooding]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     0                keywords\n",
       "0  accepting financial  [accepting, financial]\n",
       "1             affected              [affected]\n",
       "2       affected areas       [affected, areas]\n",
       "3   affected explosion   [affected, explosion]\n",
       "4    affected flooding    [affected, flooding]"
      ]
     },
     "execution_count": 583,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into lists of single words and word pairings\n",
    "keys['single'] = keys['keywords'].map(lambda x: len(x) if len(x) == 1 else 0)\n",
    "keys_single = keys[keys['single']==1]\n",
    "keys_double = keys[keys['single']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_slist = list(keys_single[0])\n",
    "keys_dlist = list(keys_double['keywords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is where we added words of our own to the existing CrisisNLP list.  This is the list we will use to filter.  \n",
    "#An enterprising user of our model could edit this list themselves very easily:\n",
    "keys_slist = ['affected',\n",
    " 'aftermath',\n",
    " 'arrest',\n",
    " 'attack',\n",
    " 'authorities',\n",
    " 'blast',\n",
    " 'bomber',\n",
    " 'bombing',\n",
    " 'braces',\n",
    " 'buried',\n",
    " 'bushfire',\n",
    " 'casualties',\n",
    " 'cleanup',\n",
    " 'collapse',\n",
    " 'collapsed',\n",
    " 'conditions',\n",
    " 'crash',\n",
    " 'crisis',\n",
    " 'damage',\n",
    " 'dangerous',\n",
    " 'dead',\n",
    " 'deadly',\n",
    " 'death',\n",
    " 'destroyed',\n",
    " 'destruction',\n",
    " 'devastating',\n",
    " 'disaster',\n",
    " 'displaced',\n",
    " 'donate',\n",
    " 'dozens',\n",
    " 'dramatic',\n",
    " 'drown',\n",
    " 'emergency',\n",
    " 'enforcement',\n",
    " 'evacuated',\n",
    " 'evacuees',\n",
    " 'events',\n",
    " 'explosion',\n",
    " 'fatalities',\n",
    " 'firefighters',\n",
    " 'flood',\n",
    " 'flooding',\n",
    " 'floodwaters',\n",
    " 'footage',\n",
    " 'help!',\n",
    " 'hurricane',\n",
    " 'imminent',\n",
    " 'impacted',\n",
    " 'injured',\n",
    " 'injuries',\n",
    " 'inundated',\n",
    " 'investigation',\n",
    " 'landfall',\n",
    " 'levy',\n",
    " 'looting',\n",
    " 'magnitude',\n",
    " 'massive',\n",
    " 'military',\n",
    " 'nursing',\n",
    " 'outage',\n",
    " 'prayers',\n",
    " 'praying',\n",
    " 'ravaged',\n",
    " 'rebuild',\n",
    " 'rebuilt',\n",
    " 'recede',\n",
    " 'recover',\n",
    " 'redcross',\n",
    " 'relief',\n",
    " 'rescue',\n",
    " 'rescuers',\n",
    " 'residents',\n",
    " 'responders',\n",
    " 'rubble',\n",
    " 'saddened',\n",
    " 'safety',\n",
    " 'seismic',\n",
    " 'shelter',\n",
    " 'soldier',\n",
    " 'storm',\n",
    " 'stream',\n",
    " 'surviving',\n",
    " 'survivor',\n",
    " 'terrifying',\n",
    " 'terror',\n",
    " 'toll',\n",
    " 'tornado',\n",
    " 'torrential',\n",
    " 'toxins',\n",
    " 'tragedy',\n",
    " 'tragic',\n",
    " 'troops',\n",
    " 'twister',\n",
    " 'unaccounted',\n",
    " 'urgent',\n",
    " 'victims',\n",
    " 'volunteers',\n",
    " 'warning',\n",
    " 'wounded']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply Filters to Generate List of Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Figure 8 Model to Sandy Landfall Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('sandy_landfall.csv')\n",
    "\n",
    "#Cleaning\n",
    "data.dropna(subset=['id','text','created_at'], inplace=True)\n",
    "data['text'] = data['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "data['text'] = data['text'].map(lambda x: x.lower())\n",
    "data['text'] = data['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "data['text'] = data['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "data = data[data.duplicated('text')==False]\n",
    "\n",
    "#Apply VotingClassifier model to data\n",
    "with open('kaggle_model_2.pkl', 'rb') as file:  \n",
    "    model = pickle.load(file)\n",
    "data['predicted']= model.predict(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Keyword Filter to Sandy Landfall Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Isolates tweets that have not been predicted as disaster tweets by the Figure 8 model\n",
    "non_predicted = data[data['predicted']==0]\n",
    "predicted = data[data['predicted']==1]\n",
    "\n",
    "#Recall that keys_slist is the list of crisis keywords defined explicitly above\n",
    "#This maps through the tweets not labeled as disaster and labels them as disaster if they include keywords\n",
    "non_predicted['predicted'] = non_predicted['text'].map(lambda x: 1 if sum([x.find(i) + 1 for i in keys_slist])>0 else 0)\n",
    "\n",
    "#Combines keyword flags and Figure 8 model flags to produce the set of all disaster tweets\n",
    "keywords =non_predicted[non_predicted['predicted']==1]\n",
    "disaster_tweets = pd.concat([keywords,predicted], ignore_index=True)\n",
    "\n",
    "#Produces set of all regular tweets\n",
    "regular_tweets = non_predicted[non_predicted['predicted']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets.to_csv('disaster_tweets.csv')\n",
    "regular_tweets.to_csv('regular_tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Identifying Critical Tweets Among Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually Labeling Disaster Related Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combed through the approximately 9000 tweets identified by the Figure Eight Model and Keyword Filtering as disaster tweets. We manually identified about 900 tweets that we felt met criteria for \"critical\" - i.e.; novel information that could be immediately relevant to emergency responders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 773,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload disaster tweets with labels now included\n",
    "disaster_labeled = pd.read_csv('./manual_tags_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 774,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263097193563566080</td>\n",
       "      <td>rt @passantino: wow: floodwaters inundate grou...</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.7245571,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.025018, -...</td>\n",
       "      <td>{'id': 'de599025180e2ee7', 'url': 'https://api...</td>\n",
       "      <td>{'id': 373792493, 'id_str': '373792493', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263097199213285377</td>\n",
       "      <td>some folks maybe feelin lonely bein in a storm...</td>\n",
       "      <td>Tue Oct 30 01:57:15 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-76.8325555,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [38.89184242,...</td>\n",
       "      <td>{'id': '19f2fcdf0d209467', 'url': 'https://api...</td>\n",
       "      <td>{'id': 250905822, 'id_str': '250905822', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263097201469845504</td>\n",
       "      <td>#itjustgotreal ... #iphonealerts  @ #hurricane...</td>\n",
       "      <td>Tue Oct 30 01:57:15 +0000 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 'b6ea2e341ba4356f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 112052977, 'id_str': '112052977', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'itJustGotReal', 'indic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>und</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263097206125518849</td>\n",
       "      <td>@ahurricanesandy hey #sandy get your ass down ...</td>\n",
       "      <td>Tue Oct 30 01:57:16 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-79.99661002...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [35.97201904,...</td>\n",
       "      <td>{'id': 'aef8c3da277ca498', 'url': 'https://api...</td>\n",
       "      <td>{'id': 266676966, 'id_str': '266676966', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'Sandy', 'indices': [21...</td>\n",
       "      <td>364217289.0</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263097208923099136</td>\n",
       "      <td>everytime the wind picks up it sounds like som...</td>\n",
       "      <td>Tue Oct 30 01:57:17 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-80.4487703,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [37.213948, -...</td>\n",
       "      <td>{'id': '820684853e0f1eb6', 'url': 'https://api...</td>\n",
       "      <td>{'id': 479075523, 'id_str': '479075523', 'name...</td>\n",
       "      <td>{'hashtags': [{'text': 'hurricanessuck', 'indi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263097193563566080  rt @passantino: wow: floodwaters inundate grou...   \n",
       "1  263097199213285377  some folks maybe feelin lonely bein in a storm...   \n",
       "2  263097201469845504  #itjustgotreal ... #iphonealerts  @ #hurricane...   \n",
       "3  263097206125518849  @ahurricanesandy hey #sandy get your ass down ...   \n",
       "4  263097208923099136  everytime the wind picks up it sounds like som...   \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Oct 30 01:57:13 +0000 2012   \n",
       "1  Tue Oct 30 01:57:15 +0000 2012   \n",
       "2  Tue Oct 30 01:57:15 +0000 2012   \n",
       "3  Tue Oct 30 01:57:16 +0000 2012   \n",
       "4  Tue Oct 30 01:57:17 +0000 2012   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0  {'type': 'Point', 'coordinates': [-80.7245571,...   \n",
       "1  {'type': 'Point', 'coordinates': [-76.8325555,...   \n",
       "2                                                NaN   \n",
       "3  {'type': 'Point', 'coordinates': [-79.99661002...   \n",
       "4  {'type': 'Point', 'coordinates': [-80.4487703,...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0  {'type': 'Point', 'coordinates': [41.025018, -...   \n",
       "1  {'type': 'Point', 'coordinates': [38.89184242,...   \n",
       "2                                                NaN   \n",
       "3  {'type': 'Point', 'coordinates': [35.97201904,...   \n",
       "4  {'type': 'Point', 'coordinates': [37.213948, -...   \n",
       "\n",
       "                                               place  \\\n",
       "0  {'id': 'de599025180e2ee7', 'url': 'https://api...   \n",
       "1  {'id': '19f2fcdf0d209467', 'url': 'https://api...   \n",
       "2  {'id': 'b6ea2e341ba4356f', 'url': 'https://api...   \n",
       "3  {'id': 'aef8c3da277ca498', 'url': 'https://api...   \n",
       "4  {'id': '820684853e0f1eb6', 'url': 'https://api...   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'id': 373792493, 'id_str': '373792493', 'name...   \n",
       "1  {'id': 250905822, 'id_str': '250905822', 'name...   \n",
       "2  {'id': 112052977, 'id_str': '112052977', 'name...   \n",
       "3  {'id': 266676966, 'id_str': '266676966', 'name...   \n",
       "4  {'id': 479075523, 'id_str': '479075523', 'name...   \n",
       "\n",
       "                                            entities  in_reply_to_user_id  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "1  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "2  {'hashtags': [{'text': 'itJustGotReal', 'indic...                  NaN   \n",
       "3  {'hashtags': [{'text': 'Sandy', 'indices': [21...          364217289.0   \n",
       "4  {'hashtags': [{'text': 'hurricanessuck', 'indi...                  NaN   \n",
       "\n",
       "  lang  predicted  tag  \n",
       "0   en        1.0  1.0  \n",
       "1   en        1.0  0.0  \n",
       "2  und        1.0  0.0  \n",
       "3   en        1.0  0.0  \n",
       "4   en        1.0  0.0  "
      ]
     },
     "execution_count": 774,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **tag** column indicates whether the tweet was manually tagged as \"critical\".  This is based on our first tagging run, in which we cast a relatively wide net.  The **predicted** column indicates whether the tweet is disaster related (all of these were, of course), in case we want to concatenate with our regular tweets data (predicted = 0) later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is also a good look at what our tweet data dictionaries contain.  Not everything is immediately relevant to the project here, but we left in anything that might be used predictively later on.  Obviously, we have the specific ID that we can use to look up the tweet online, or use as an index.  We also have:\n",
    "\n",
    "- the text of the tweet\n",
    "- the timestamp\n",
    "- the geo-coordinate information\n",
    "- a place dictionary that contains information about the area including the city/neighborhood of origin\n",
    "- a dictionary of information about the user sending the tweet\n",
    "- a dictionary that grabs any hashtags the tweet contains\n",
    "- a column that allows us to tell if the tweet was a reply or not\n",
    "- the language of the tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Disaster Tweets - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 776,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>263097391169826817</td>\n",
       "      <td>@rwzombie fuck #hurricanesandy keep voting</td>\n",
       "      <td>Tue Oct 30 01:58:00 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-74.95308309...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.04676075,...</td>\n",
       "      <td>{'id': '31fbce652077706d', 'url': 'https://api...</td>\n",
       "      <td>{'id': 25303398, 'id_str': '25303398', 'name':...</td>\n",
       "      <td>{'hashtags': [{'text': 'hurricanesandy', 'indi...</td>\n",
       "      <td>43469093.0</td>\n",
       "      <td>en</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                                        text  \\\n",
       "30  263097391169826817  @rwzombie fuck #hurricanesandy keep voting   \n",
       "\n",
       "                        created_at  \\\n",
       "30  Tue Oct 30 01:58:00 +0000 2012   \n",
       "\n",
       "                                          coordinates  \\\n",
       "30  {'type': 'Point', 'coordinates': [-74.95308309...   \n",
       "\n",
       "                                                  geo  \\\n",
       "30  {'type': 'Point', 'coordinates': [40.04676075,...   \n",
       "\n",
       "                                                place  \\\n",
       "30  {'id': '31fbce652077706d', 'url': 'https://api...   \n",
       "\n",
       "                                                 user  \\\n",
       "30  {'id': 25303398, 'id_str': '25303398', 'name':...   \n",
       "\n",
       "                                             entities  in_reply_to_user_id  \\\n",
       "30  {'hashtags': [{'text': 'hurricanesandy', 'indi...           43469093.0   \n",
       "\n",
       "   lang  predicted  tag  \n",
       "30   en        1.0  NaN  "
      ]
     },
     "execution_count": 776,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if we missed or deleted any cells while tagging manually\n",
    "disaster_labeled[disaster_labeled['tag'].isnull()==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 777,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    8694\n",
       "1.0     836\n",
       "Name: tag, dtype: int64"
      ]
     },
     "execution_count": 777,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled['tag'] = disaster_labeled['tag'].fillna(0)\n",
    "disaster_labeled['tag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 780,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled.dropna(subset=['id','text','created_at'],inplace=True)\n",
    "\n",
    "#english language only\n",
    "disaster_labeled = disaster_labeled[disaster_labeled['lang']=='en']\n",
    "#create readable datetime column and sort by datetime\n",
    "disaster_labeled['datetime'] = pd.to_datetime(disaster_labeled['created_at'])\n",
    "disaster_labeled = disaster_labeled.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "disaster_labeled = disaster_labeled[['id','text','datetime','geo','predicted','tag']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "disaster_labeled['text'] = disaster_labeled['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "disaster_labeled.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "disaster_labeled['text'] = disaster_labeled['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "disaster_labeled.dropna(subset=['text'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>geo</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>@godseyg i was arrested about 36 hours later. ...</td>\n",
       "      <td>2012-10-30 00:00:02</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12081393,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>wish i was wiff my love during this disaster #...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.28866479,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>im a hurricane vet.. so #hurricanesandy isnt a...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12088316,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>the scorpions - rock you like a hurricane #201...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.60095122,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>lights out @ frankenstorm apocalypse - hurrica...</td>\n",
       "      <td>2012-10-30 00:00:06</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.79093941,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  2.630677e+17  @godseyg i was arrested about 36 hours later. ...   \n",
       "1  2.630677e+17  wish i was wiff my love during this disaster #...   \n",
       "2  2.630677e+17  im a hurricane vet.. so #hurricanesandy isnt a...   \n",
       "3  2.630677e+17  the scorpions - rock you like a hurricane #201...   \n",
       "4  2.630677e+17  lights out @ frankenstorm apocalypse - hurrica...   \n",
       "\n",
       "              datetime                                                geo  \\\n",
       "0  2012-10-30 00:00:02  {'type': 'Point', 'coordinates': [39.12081393,...   \n",
       "1  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [41.28866479,...   \n",
       "2  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [39.12088316,...   \n",
       "3  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [42.60095122,...   \n",
       "4  2012-10-30 00:00:06  {'type': 'Point', 'coordinates': [40.79093941,...   \n",
       "\n",
       "   predicted  tag  \n",
       "0        1.0  0.0  \n",
       "1        1.0  0.0  \n",
       "2        1.0  0.0  \n",
       "3        1.0  0.0  \n",
       "4        1.0  0.0  "
      ]
     },
     "execution_count": 865,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled.to_csv('./disaster_labeled.csv',index=False)\n",
    "\n",
    "disaster_labeled[disaster_labeled['tag']==1].to_csv('./critical.csv', index=False)\n",
    "disaster_labeled[disaster_labeled['tag']==0].to_csv('./disaster_nonrel.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighting and Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After manually tagging, we had about a 9:1 ratio of disaster non critical to disaster critical tweets, so baseline accuracy of about 90%.  In order to resolve this it was necessary to oversample the tweets we had labeled critical (bootstrapping).  We expanded the 900ish critical tweets to 9000, balancing the classes evenly.\n",
    "\n",
    "While manually tagging, we had often felt that some of the tweets we labeled critical were borderline, while others were immediate and dire, and highly useful potentially.  We felt this should be reflected in our bootstrapping by using weighted probabilities in our oversampling.  We thus went through the 900 critical tweets and weighted them 1, 3, 5, or 10 based on their degree of relevance to emergency personnel who might be scanning twitter for information.  We then normalized these to percentages of 1 to create probabilities for use in bootstrapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted = pd.read_csv('weighted_critical.csv')\n",
    "\n",
    "#Converts manually assigned weights to bootstrap weighted probabilities\n",
    "weighted['weight']= weighted['weight'].map(lambda x: x/weighted['weight'].sum())\n",
    "\n",
    "boot = weighted.sample(9000,replace=True, weights=weighted['weight'])\n",
    "boot.drop(columns = ['weight'],inplace=True)\n",
    "\n",
    "nonrel = pd.read_csv('disaster_nonrel.csv')\n",
    "nonrel.drop(columns='Unnamed: 0', inplace=True)\n",
    "\n",
    "disaster = pd.concat([nonrel, boot], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model to Predict Actual Relevance from within Disaster Related Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tried our hand with a few different things, but ended up settling on a similarly trained VotingClassifier to our earlier Figure 8 Model, with combination of trained LogReg, XGBoost and Random Forest Classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = disaster['text']\n",
    "y = disaster['tag']\n",
    "\n",
    "disaster_model = Pipeline([\n",
    "        ('count_vect', CountVectorizer(min_df=2,  \n",
    "                                  ngram_range=(1, 3))),     \n",
    "        ('clf', VotingClassifier(estimators=[(\"pip1\", LogisticRegression(penalty='l2', C=0.1)),\n",
    "                                  (\"pip2\", XGBClassifier(n_estimators=1500, min_samples_split = 2, max_depth= 3)), \n",
    "                                  (\"pip3\", RandomForestClassifier(max_depth=75, min_samples_split=4, n_estimators=200))]\n",
    "                                    ,voting='soft'))\n",
    "         ])\n",
    "disaster_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observed cross-validation scores in the mid to high 90s with this training process, however this was obviously the result of highly weighted bootstrapped replacement tweets appearing in the test split as well as in the train split.  The only way to really assess performance is to run the model on test data and see how its predictions perform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordCloud: Words that Help Identify Critical Tweets Among Disaster Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/FL6iEue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing 2-Phase Model {1. Disaster Filter, 2. Predict Critical} on New Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building ID List for Sandy Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our test set, we wanted to sample from a wider section of the hurricane's course.  We aimed for about 40000 tweets for the test set.  We decided to randomly select 40000 tweets from the 2 million or so post-landfall time window tweets, so, starting at the end of the window of our training set (10PM on 10/29) through the last timestamp in the ID List (11/2). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates our test set id list\n",
    "#Random selection of tweets over the rest of the hurricane, so we can show geographic progression in mapping as well\n",
    "#40,500 was chosen as n because it is a convenient multiple of 2700 (900*3) for the tweet pulls\n",
    "\n",
    "sandy_random = dftime.loc[4608435:6000000,:]\n",
    "sandy_random = sandy_random.sample(40500,replace=False,random_state=37)\n",
    "sandy_random = sandy_random.sort_values('datetime')\n",
    "sandy_random = sandy_random.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_random.to_csv('sandy_random.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the main set, we ran tweet pulls in tandem and combined the resulting csvs to complete the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 758,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = pd.read_csv('./alexrandom1.csv')\n",
    "test2 = pd.read_csv('./alexrandom2.csv')\n",
    "test3 = pd.read_csv('./eamonrandom1.csv')\n",
    "test4 = pd.read_csv('./eamonrandom2.csv')\n",
    "test5 = pd.read_csv('./eamonrandom3.csv')\n",
    "test6 = pd.read_csv('./javirandom1.csv')\n",
    "test7 = pd.read_csv('./javirandom2.csv')\n",
    "test8 = pd.read_csv('./javirandom3.csv')\n",
    "test9 = pd.read_csv('./javirandom4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 759,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = pd.concat([test1,test2,test3,test4,test5,test6,test7,test8,test9],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 760,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.dropna(subset=['id','text','created_at'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = testset[testset['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "testset['datetime'] = pd.to_datetime(testset['created_at'])\n",
    "testset = testset.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#english language only\n",
    "testset = testset[testset['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "testset['datetime'] = pd.to_datetime(testset['created_at'])\n",
    "testset = testset.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "testset = testset[['id','text','datetime','geo']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "testset['text'] = testset['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "testset.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "testset['text'] = testset['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "testset.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "testset['id'] = testset['id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset['text'] = testset['text'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "testset['text']= testset['text'].map(lambda x: x.lower())\n",
    "testset['text'] = testset['text'].apply(lambda x: re.split('http:\\/\\/.*', str(x))[0])\n",
    "testset['text'] = testset['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
    "\n",
    "testset.to_csv('testset.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the Filter/Models on the Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Figure 8 model and separate into non-predicted and disaster tweets.\n",
    "testset['is_disaster']= kaggle_model.predict(testset['text'])\n",
    "regular_test = testset[testset['is_disaster']==0]\n",
    "disaster_test = testset[testset['is_disaster']==1]\n",
    "\n",
    "#Apply keyword filter to non-predicted to identify additional disaster tweets, and combine to collect all disaster tweets\n",
    "regular_test['is_disaster'] = regular_test['text'].map(lambda x: 1 if sum([x.find(i) + 1 for i in keys_slist])>0 else 0)\n",
    "keywords =regular_test[regular_test['is_disaster']==1]\n",
    "disaster_test = pd.concat([keywords,disaster_test], ignore_index=True)\n",
    "\n",
    "#Run Critical Tweet Identifier model on disaster tweets\n",
    "disaster_test['critical']= disaster_model.predict(disaster_test['text'])\n",
    "\n",
    "#Review each class\n",
    "critical = disaster_test[disaster_test['critical'] == 1]\n",
    "disaster nonrel = disaster_test[disaster_test['critical'] == 0]\n",
    "regular = regular_test[regular_test['is_disaster']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show results (I.E. disaster non-relevant and disaster critical results) side by side...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remark on how the results were actually quite satisfying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Combined Landfall Dataset with Class Labeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One alternative that we considered, with our manual tags done, would simply be to add regular tweets back in and train on the entire sea of tweets, regular and disaster related, trying to pick out only the actually relevant tweets from everything.  We felt this would be less powerful than using a second phase to specifically discriminate actually relevant tweets from non-relevant disaster-related tweets.  But we may want to try this at some point.  It does have the advantage of condensing the predictive process into a single step.  \n",
    "\n",
    "Even without this application though, it's necessary for us to build out a cleaned, combined dataset to be able to visualize a geo-map where we can easily represent disaster and critical tweets amid the sea of all tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the set of Sandy Landfall Tweets (our main pull) that was not labeled \n",
    "#as disaster tweets by either the Figure 8 Model or the Keyword Filtering\n",
    "regular_tweets = pd.read_csv('./regular_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>created_at</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>geo</th>\n",
       "      <th>place</th>\n",
       "      <th>user</th>\n",
       "      <th>entities</th>\n",
       "      <th>in_reply_to_user_id</th>\n",
       "      <th>lang</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263097187775434752</td>\n",
       "      <td>if anybody needs a 2nd shift job and can pass ...</td>\n",
       "      <td>Tue Oct 30 01:57:12 +0000 2012</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{'id': 146665477, 'id_str': '146665477', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263097188446523393</td>\n",
       "      <td>of course when the voice is on, my tv decides ...</td>\n",
       "      <td>Tue Oct 30 01:57:12 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-81.33815822...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.15005871,...</td>\n",
       "      <td>{'id': '45a0ea3329c38f9f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 65144874, 'id_str': '65144874', 'name':...</td>\n",
       "      <td>{'hashtags': [{'text': 'ThanksSandy', 'indices...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263097191260901378</td>\n",
       "      <td>@ken_fedor oh hell yeah. we need too</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-74.10466037...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.87194808,...</td>\n",
       "      <td>{'id': '86fc60f26e1639cc', 'url': 'https://api...</td>\n",
       "      <td>{'id': 331299957, 'id_str': '331299957', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263097191881646080</td>\n",
       "      <td>ok...  enough sandy, time to go away.  no real...</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-77.092894, ...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [38.978183, -...</td>\n",
       "      <td>{'id': '864ff125241f172f', 'url': 'https://api...</td>\n",
       "      <td>{'id': 302586627, 'id_str': '302586627', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263097193530023936</td>\n",
       "      <td>niggas not loyal</td>\n",
       "      <td>Tue Oct 30 01:57:13 +0000 2012</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-81.6315227,...</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.5384851, ...</td>\n",
       "      <td>{'id': '0eb9676d24b211f1', 'url': 'https://api...</td>\n",
       "      <td>{'id': 390423015, 'id_str': '390423015', 'name...</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>en</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263097187775434752  if anybody needs a 2nd shift job and can pass ...   \n",
       "1  263097188446523393  of course when the voice is on, my tv decides ...   \n",
       "2  263097191260901378               @ken_fedor oh hell yeah. we need too   \n",
       "3  263097191881646080  ok...  enough sandy, time to go away.  no real...   \n",
       "4  263097193530023936                                  niggas not loyal    \n",
       "\n",
       "                       created_at  \\\n",
       "0  Tue Oct 30 01:57:12 +0000 2012   \n",
       "1  Tue Oct 30 01:57:12 +0000 2012   \n",
       "2  Tue Oct 30 01:57:13 +0000 2012   \n",
       "3  Tue Oct 30 01:57:13 +0000 2012   \n",
       "4  Tue Oct 30 01:57:13 +0000 2012   \n",
       "\n",
       "                                         coordinates  \\\n",
       "0                                                NaN   \n",
       "1  {'type': 'Point', 'coordinates': [-81.33815822...   \n",
       "2  {'type': 'Point', 'coordinates': [-74.10466037...   \n",
       "3  {'type': 'Point', 'coordinates': [-77.092894, ...   \n",
       "4  {'type': 'Point', 'coordinates': [-81.6315227,...   \n",
       "\n",
       "                                                 geo  \\\n",
       "0                                                NaN   \n",
       "1  {'type': 'Point', 'coordinates': [41.15005871,...   \n",
       "2  {'type': 'Point', 'coordinates': [40.87194808,...   \n",
       "3  {'type': 'Point', 'coordinates': [38.978183, -...   \n",
       "4  {'type': 'Point', 'coordinates': [41.5384851, ...   \n",
       "\n",
       "                                               place  \\\n",
       "0                                                NaN   \n",
       "1  {'id': '45a0ea3329c38f9f', 'url': 'https://api...   \n",
       "2  {'id': '86fc60f26e1639cc', 'url': 'https://api...   \n",
       "3  {'id': '864ff125241f172f', 'url': 'https://api...   \n",
       "4  {'id': '0eb9676d24b211f1', 'url': 'https://api...   \n",
       "\n",
       "                                                user  \\\n",
       "0  {'id': 146665477, 'id_str': '146665477', 'name...   \n",
       "1  {'id': 65144874, 'id_str': '65144874', 'name':...   \n",
       "2  {'id': 331299957, 'id_str': '331299957', 'name...   \n",
       "3  {'id': 302586627, 'id_str': '302586627', 'name...   \n",
       "4  {'id': 390423015, 'id_str': '390423015', 'name...   \n",
       "\n",
       "                                            entities  in_reply_to_user_id  \\\n",
       "0  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "1  {'hashtags': [{'text': 'ThanksSandy', 'indices...                  NaN   \n",
       "2  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "3  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "4  {'hashtags': [], 'symbols': [], 'user_mentions...                  NaN   \n",
       "\n",
       "  lang  predicted  tag  \n",
       "0   en          0    0  \n",
       "1   en          0    0  \n",
       "2   en          0    0  \n",
       "3   en          0    0  \n",
       "4   en          0    0  "
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Tweets - Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates target column for regular tweets, all of which should be 0\n",
    "regular_tweets['tag'] = np.zeros(len(regular_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105804, 12)"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                         0\n",
       "text                       4\n",
       "created_at                 2\n",
       "coordinates            10378\n",
       "geo                    10378\n",
       "place                   3170\n",
       "user                      24\n",
       "entities                   2\n",
       "in_reply_to_user_id    69816\n",
       "lang                      27\n",
       "predicted                  0\n",
       "tag                        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regular_tweets.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_tweets.dropna(subset=['text','created_at'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english language only\n",
    "regular_tweets = regular_tweets[regular_tweets['lang']=='en']\n",
    "\n",
    "#create readable datetime column and sort by datetime\n",
    "regular_tweets['datetime'] = pd.to_datetime(regular_tweets['created_at'])\n",
    "regular_tweets = regular_tweets.sort_values('datetime').reset_index(drop=True)\n",
    "\n",
    "#Selects columns of interest\n",
    "regular_tweets = regular_tweets[['id','text','datetime','geo','predicted','tag']]\n",
    "\n",
    "#remove retweets (begins with rt)\n",
    "regular_tweets['text'] = regular_tweets['text'].map(lambda x: np.nan if x.find('rt')==0 else x)\n",
    "regular_tweets.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "#remove retweets (contains rt elsewhere)\n",
    "regular_tweets['text'] = regular_tweets['text'].map(lambda x: np.nan if 'rt' in x.split(' ') else x)\n",
    "regular_tweets.dropna(subset=['text'],inplace=True)\n",
    "\n",
    "regular_tweets['id'] = regular_tweets['id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Regular and Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_labeled = pd.read_csv('disaster_labeled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>datetime</th>\n",
       "      <th>geo</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>@godseyg i was arrested about 36 hours later. ...</td>\n",
       "      <td>2012-10-30 00:00:02</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12081393,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>wish i was wiff my love during this disaster #...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [41.28866479,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>im a hurricane vet.. so #hurricanesandy isnt a...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [39.12088316,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>the scorpions - rock you like a hurricane #201...</td>\n",
       "      <td>2012-10-30 00:00:04</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [42.60095122,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.630677e+17</td>\n",
       "      <td>lights out @ frankenstorm apocalypse - hurrica...</td>\n",
       "      <td>2012-10-30 00:00:06</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [40.79093941,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  \\\n",
       "0  2.630677e+17  @godseyg i was arrested about 36 hours later. ...   \n",
       "1  2.630677e+17  wish i was wiff my love during this disaster #...   \n",
       "2  2.630677e+17  im a hurricane vet.. so #hurricanesandy isnt a...   \n",
       "3  2.630677e+17  the scorpions - rock you like a hurricane #201...   \n",
       "4  2.630677e+17  lights out @ frankenstorm apocalypse - hurrica...   \n",
       "\n",
       "              datetime                                                geo  \\\n",
       "0  2012-10-30 00:00:02  {'type': 'Point', 'coordinates': [39.12081393,...   \n",
       "1  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [41.28866479,...   \n",
       "2  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [39.12088316,...   \n",
       "3  2012-10-30 00:00:04  {'type': 'Point', 'coordinates': [42.60095122,...   \n",
       "4  2012-10-30 00:00:06  {'type': 'Point', 'coordinates': [40.79093941,...   \n",
       "\n",
       "   predicted  tag  \n",
       "0        1.0  0.0  \n",
       "1        1.0  0.0  \n",
       "2        1.0  0.0  \n",
       "3        1.0  0.0  \n",
       "4        1.0  0.0  "
      ]
     },
     "execution_count": 821,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combines disaster-related and regular tweets into a cleaned dataset containing all three classes\n",
    "#(Regular, Disaster Non-Critical, and Disaster Critical)\n",
    "#The convenience of this dataset is it can easily be manipulated to have a class column with all three types labeled.\n",
    "#This will be useful for geo-mapping visualizations.\n",
    "sandy_combined = pd.concat([disaster_labeled,regular_tweets],ignore_index=True)\n",
    "sandy_combined['datetime'] = pd.to_datetime(sandy_combined['datetime'])\n",
    "sandy_combined = sandy_combined.sort_values('datetime')\n",
    "sandy_combined['id'] = sandy_combined['id'].map(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_combined.to_csv('./sandy_combined.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geomapping Landfall Data in Tableau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first geomap, we want to map the full body of tweets (regular, disaster, and critical) from the main pull, ie all geotagged tweets from the northeastern seaboard from the time of Sandy landfall in NJ/NY to about 15 hours later.  Some additional processing needs to be done.  Also, not all tweets in our pull have precise coordinates, although most do.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geomap Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap = pd.read_csv('sandy_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove all rows that don't have coordinates\n",
    "sandy_geomap = sandy_geomap[sandy_geomap['geo'].notnull()==True]\n",
    "\n",
    "#Splits geo column into latitude and longitude columns\n",
    "sandy_geomap['geo'] = sandy_geomap['geo'].map(lambda x: x.split('[')[1].split(']')[0])\n",
    "sandy_geomap['latitude'] = sandy_geomap['geo'].map(lambda x: x.split(',')[0])\n",
    "sandy_geomap['longitude'] = sandy_geomap['geo'].map(lambda x: x.split(',')[1])\n",
    "sandy_geomap['latitude'] = sandy_geomap['latitude'].map(lambda x: float(x))\n",
    "sandy_geomap['longitude'] = sandy_geomap['longitude'].map(lambda x: float(x))\n",
    "\n",
    "#Condenses to relevant information for geomapping\n",
    "sandy_geomap = sandy_geomap[['id','text','latitude','longitude','datetime','predicted','tag']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 94043 entries, 0 to 104272\n",
      "Data columns (total 7 columns):\n",
      "id           94043 non-null int64\n",
      "text         94043 non-null object\n",
      "latitude     94043 non-null float64\n",
      "longitude    94043 non-null float64\n",
      "datetime     94043 non-null object\n",
      "predicted    94043 non-null float64\n",
      "tag          94043 non-null float64\n",
      "dtypes: float64(4), int64(1), object(2)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "sandy_geomap.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap['class'] = sandy_geomap['predicted'] + sandy_geomap['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    86060\n",
       "1.0     7297\n",
       "2.0      686\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 848,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_geomap['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "sandy_geomap.to_csv('sandy_geomap.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94043"
      ]
     },
     "execution_count": 850,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sandy_geomap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>datetime</th>\n",
       "      <th>predicted</th>\n",
       "      <th>tag</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263067699821821952</td>\n",
       "      <td>i should probably start doing my hw</td>\n",
       "      <td>43.104663</td>\n",
       "      <td>-75.127981</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>263067700270596096</td>\n",
       "      <td>good thing i practiced my i totally understand...</td>\n",
       "      <td>39.166361</td>\n",
       "      <td>-84.606048</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263067699133947904</td>\n",
       "      <td>@day_hammonds you already know</td>\n",
       "      <td>34.997820</td>\n",
       "      <td>-80.090215</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>263067696235692032</td>\n",
       "      <td>@tiffany_niccole im sitting at this gate a6 to...</td>\n",
       "      <td>39.998155</td>\n",
       "      <td>-82.884330</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263067699213660160</td>\n",
       "      <td>girls who go commandogtgt</td>\n",
       "      <td>39.923918</td>\n",
       "      <td>-75.173551</td>\n",
       "      <td>2012-10-30 00:00:01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   id                                               text  \\\n",
       "0  263067699821821952                i should probably start doing my hw   \n",
       "1  263067700270596096  good thing i practiced my i totally understand...   \n",
       "2  263067699133947904                    @day_hammonds you already know    \n",
       "3  263067696235692032  @tiffany_niccole im sitting at this gate a6 to...   \n",
       "4  263067699213660160                          girls who go commandogtgt   \n",
       "\n",
       "    latitude  longitude             datetime  predicted  tag  class  \n",
       "0  43.104663 -75.127981  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "1  39.166361 -84.606048  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "2  34.997820 -80.090215  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "3  39.998155 -82.884330  2012-10-30 00:00:01        0.0  0.0    0.0  \n",
       "4  39.923918 -75.173551  2012-10-30 00:00:01        0.0  0.0    0.0  "
      ]
     },
     "execution_count": 867,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sandy_geomap.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Disaster-Related and Critical Tweets - Hurricane Sandy Landfall - Northeast Region (Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class='tableauPlaceholder' id='viz1547848544212' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848544212');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div class='tableauPlaceholder' id='viz1547848544212' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ma&#47;MappingDisasterRelatedandDisasterUrgentTweets-HurricaneSandyLandfall&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848544212');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-Time Mapping Disaster Critical Tweets in the NYC Area during Hurricane Sandy Landfall (Tableau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class='tableauPlaceholder' id='viz1547848675634' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NYCSandyTweetMap&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848675634');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<div class='tableauPlaceholder' id='viz1547848675634' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1_rss.png' style='border: none' /></a></noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' /> <param name='embed_code_version' value='3' /> <param name='site_root' value='' /><param name='name' value='NYCSandyTweetMap&#47;Sheet1' /><param name='tabs' value='no' /><param name='toolbar' value='yes' /><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;NY&#47;NYCSandyTweetMap&#47;Sheet1&#47;1.png' /> <param name='animate_transition' value='yes' /><param name='display_static_image' value='yes' /><param name='display_spinner' value='yes' /><param name='display_overlay' value='yes' /><param name='display_count' value='yes' /><param name='filter' value='publish=yes' /></object></div>                <script type='text/javascript'>                    var divElement = document.getElementById('viz1547848675634');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                </script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Streaming Disaster Tweets via Twython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wanted to return at the end to the root purpose of the project and see what we could do with applying our model to real-time tweets. The primary difficulty here was just obtaining the tweets, but we were able to develop a method using Twython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This set of code uses Twython to stream tweets with disaster keywords in real-time.  \n",
    "#It also compiles these incoming tweets into dataframes of n=100, for potential analysis with our model.\n",
    "\n",
    "from twython import TwythonStreamer\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "class MyStreamer(TwythonStreamer):\n",
    "    def on_success(self, data, ids=[], texts=[],geos=[],created_ats=[],df = pd.DataFrame(columns=['id','text','geo','created_at']),count=0 ):\n",
    "        print(data['text'])\n",
    "        now = datetime.datetime.now()\n",
    "        ids.append(data['id'])\n",
    "        texts.append(data['text'])\n",
    "        geos.append(data['geo'])\n",
    "        created_ats.append(data['created_at'])\n",
    "        count = len(texts)\n",
    "        print(count)\n",
    "        if count == 10:\n",
    "            df['id'] = ids\n",
    "            df['text'] = texts\n",
    "            df['geo'] = geos\n",
    "            df['created_at'] = created_ats\n",
    "            df.to_csv(\"./Live_tweets/Live Tweets \"+ str(now)+\".csv\")\n",
    "            print('CSV SAVED')\n",
    "            count = 0\n",
    "            df = pd.DataFrame(columns=['id','text','geo','created_at'])\n",
    "        \n",
    "stream = MyStreamer(app_key='Ed2Wq0e44bvHtrsmZGggAs3Lx',\n",
    "                    app_secret='LhSDbftqQXjrQIZK9z5BqIOVoZjmEejsliMyt6C4xeHbjecCst',\n",
    "                    oauth_token='269947090-jBfugJosB1EGnwu5rAFH1a6Ehrc6SAt1T2TjrR9b',\n",
    "                    oauth_token_secret='rQffeEZKG62z7WIRmtFWHKIWZkiJtkU4CfJyvlhrmSwMx')\n",
    "\n",
    "\n",
    "stream.statuses.filter(track= ['fire'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our primary takeaways in this project thus far are as follows:\n",
    "\n",
    "Despite initial setbacks, we were able to arrive at a satisfying and effective two-phase process for identifying critical disaster tweets out of the sea of all incoming tweets on social media. We were pleased with how well we were able to isolate potentially relevant tweets from a sea of test data. We also successfully visualized what an interface for receiving geotagged tweets in real-time might look like, although we expect a more integrated and multi-functional mapping software than Tableau might be necessary for real-world implementation. Finally, we were able to demonstrate proof-of-concept on live-streaming capture of tweets. \n",
    "\n",
    "Given unrestricted access to the data available via Twitter, let alone Facebook, Snapchat, Instagram (all of which FEMA or a similar organization would likely have in the hypothetical scenario where they would implement this process), I feel we have demonstrated that it would be absolutely possible to build out a very useful and accurate geo-feed of emergency response information in the area of an ongoing disaster.\n",
    "\n",
    "All of this bodes well for future expansion of the project. Other directions we might go with more time include attempting to involve another social media platform, or locate a database of tweets from a disaster scenario other than a hurricane to try and diversify our filtering process. We also would like to try and improve predictive accuracy by use of Words2Vec, which is well equipped to locate similar types of tweets (e.g. critical tweets) without explicit labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
